---
title: "Session 1.1: Bayesian thinking"
params: 
   conference: "Spatial and Spatio-Temporal Bayesian Models with `R-INLA`"
   location: ""
   date: ""
   short_title: ""
output:
  xaringan::moon_reader: 
    includes: 
       # This line adds a logo based on the format selected in the file 'assets/include_logo.html'
       in_header: "assets/latex_macros.html" 
       # NB: the actual options (eg placement of the logo and actual logo file) can be changed there
       after_body: "assets/insert-logo.html"
    seal: false
    yolo: no
    lib_dir: libs
    nature:
      beforeInit: ["https://platform.twitter.com/widgets.js"]
      navigation:
        scroll: false # disable slide transitions by scrolling
      highlightStyle: github
      highlightLines: yes
      countIncrementalSlides: no
      ratio: '16:9'
      titleSlideClass:
      - center
      - middle
    self_contained: false 
    css:
    - "assets/beamer.css"
editor_options: 
  chunk_output_type: console
---

```{r echo=F,message=FALSE,warning=FALSE,comment=NA}
# Sources the R file with all the relevant setup and commands
source("assets/setup.R")

# Stuff from 'xaringanExtra' (https://pkg.garrickadenbuie.com/xaringanExtra)
# This allows the use of panels (from 'xaringanExtra')
xaringanExtra::use_panelset()
# This allows to copy code from the slides directly
#xaringanExtra::use_clipboard()
# This freezes the frame for when there's a gif included
#xaringanExtra::use_freezeframe()

# Defines the path to the file with the .bib entries (in case there are references)
bibfile=RefManageR::ReadBib("C:/Users/magb/Dropbox/Books/INLABook/ShortCourse/VIBASS/Session1.1/Biblio.bib",check = FALSE)
```

class: title-slide

# `r rmarkdown::metadata$title``r vspace("10px")` `r rmarkdown::metadata$subtitle`

## `r rmarkdown::metadata$author`

### `r rmarkdown::metadata$institute`    

### `r rmarkdown::metadata$params$conference`, `r rmarkdown::metadata$params$location` 

<!-- Can also separate the various components of the extra argument 'params', eg as in 
### `r paste(rmarkdown::metadata$params, collapse=", ")`
-->

`r ifelse(is.null(rmarkdown::metadata$params$date),format(Sys.Date(),"%e %B %Y"),rmarkdown::metadata$params$date)`

`r include_fig("LogoMRC.png")`

---

layout: true  

.my-footer[ 
.alignleft[ 
&nbsp; &copy; Marta Blangiardo | Monica Pirani 
]

.alignright[
`r rmarkdown::metadata$params$conference`, `r short_date` 
]
] 

```{css,echo=FALSE, eval=FALSE}
.red {
  color: red;
}
.blue {
  color: 0.14 0.34 0.55;
}

.content-box-blue { background-color: #F0F8FF; }

}

.scrollable {
  height: 80%;
  overflow-y: auto;
}

```
<style>
pre {
  overflow-x: auto;
}
pre code {
  word-wrap: normal;
  white-space: pre;
}
</style>


---

# Learning objectives

After this lecture you should be able to 
`r vspace("40px")`
- Introduce Bayesian way of thinking     
`r vspace("40px")`
- Present Bayes theorem and Bayesian inference   
`r vspace("40px")`
- Describe computational methods commonly used to perform Bayesian inference     
`r vspace("40px")`

The topics treated in this lecture are presented in Chapter 3-4  of the book **Spatial and Spatio-Temporal Bayesian models with R-INLA**.

---

# Outline 

`r vspace("30px")`

1\. [Why Bayesian](#WhyBayesian)

`r vspace("30px")`

2\. [Components of a Bayesian analysis](#Components)

`r vspace("30px")`

3\. [Bayes Theorem](#BayesTheo)

`r vspace("30px")`

4\. [Bayesian computing](#BayComp)

---

name: WhyBayesian
  
`r vspace("250px")`

.myblue[.center[.huge[
**Why Bayesian**]]]

---

# Statistics - The  'Big Picture'

`r include_fig("BigPicture0.png",width="60%",title="")`

---

# Statistics - The  'Big Picture'


`r include_fig("BigPicture.png",width="60%",title="")`

`r vspace("-100px")`

- Several different ways of formulating statistical models and computing inferences from these models and data
- Can be grouped into two broad approaches:
  - Frequentist
  - Bayesian

---

# Everyday thought process

- At the start of the football season I formed a view about the chance that the team I support will be relegated (**PRIOR**), based on

  - performance last season
  
  - summer transfers
  - etc.

--

`r vspace("20px")`

- The first match is played

`r vspace("20px")`

- .red[Now I have some information from current season (**DATA**)]

--

`r vspace("20px")`

- I re-assesses the probability of relegation upwards, because
  - they lose
  - their main striker limps off

`r vspace("20px")`
  
- .red[Prior view updated with data to give **POSTERIOR** view]


---

# Thought process of a physician

- A patient presents with a set of symptoms, concerned that they might have a certain disease

`r vspace("10px")`

- The physician assesses the chance that the patient has this disease, based on

  - symptoms
  - family history
  - alternative explanations of symptoms
  - prevalence of disease

`r vspace("10px")`

- The physician sends the patient for a diagnostic test
`r vspace("10px")`
- The physician re-assesses the chance that the patient has this disease, taking account of

  - results of diagnostic test
  - reliability of diagnostic test
`r vspace("10px")`
- The physician may send the patient for further diagnostic tests

---

# Bayesian thinking

`r include_fig("Thinking1.png",width="80%",title="")`

---

# Bayesian thinking

`r include_fig("Thinking2.png",width="80%",title="")`

---

# Why Bayesian methods?

- Bayesian methods have been widely applied in many areas:
`r vspace("10px")`
  - medicine / epidemiology
  - genetics
  - ecology
  - environmental sciences
  - social and political sciences
  - finance
  - archaeology
  - .....
`r vspace("10px")`
- Motivations for adopting Bayesian approach vary:
`r vspace("10px")`
  - natural and coherent way of thinking about science and learning
  - pragmatic choice that is suitable for the problem in hand

---

# Example

A clinical trial is carried out to collect evidence about an unknown *treatment effect*

`r vspace("20px")`

.red[Conventional analysis]

- p-value for $H_0$: treatment effect is zero
`r vspace("10px")`

- Point estimate and CI as summaries of  size of treatment effect

**Aim is to learn what this trial tells us about the treatment effect**

--

`r vspace("20px")`

.red[Bayesian analysis]

- Inference is based on probability statements summarising the posterior distribution of the treatment effect

Asks: **how should this trial change our opinion about the treatment effect?**

---

name: Components
  
`r vspace("250px")`

.myblue[.center[.huge[
**Components of a Bayesian analysis**]]]

---

# Components of a Bayesian analysis

A clinical trial is carried out to collect evidence about an
unknown *treatment effect*. The Bayesian analyst needs to explicitly state
`r vspace("10px")`

- a reasonable opinion concerning the plausibility of different values of the treatment effect *excluding* the evidence from the trial (the .red[prior distribution])
`r vspace("10px")`

- the support for different values of the treatment effect based *solely* on data from the
   trial (the .red[likelihood]),

`r vspace("10px")`

and to combine these two sources to produce

- a final opinion about the treatment effect (the .red[posterior distribution])

--

`r vspace("20px")`


The final combination is done using **Bayes theorem** (and only simple rules of probability), which essentially weights the likelihood from the trial with the relative plausibilities defined by the prior distribution

`r vspace("20px")`


.content-box-green[
One can view the Bayesian approach as a formalisation of the process
of learning from experience]

---

# Bayesian inference: the posterior distribution

Posterior distribution forms basis for all inference --- can
be summarised to provide
`r vspace("10px")`
- point and interval estimates of Quantities of Interest (QOI), e.g. treatment effect, small area estimates, ...
`r vspace("10px")`
- point and interval estimates of any function of the parameters
`r vspace("10px")`
- probability that QOI (e.g. treatment effect) exceeds a critical threshold
`r vspace("10px")`
- prediction of QOI in a new unit
`r vspace("10px")`
- prior information for future experiments, trials, surveys, ...
`r vspace("10px")`
- inputs for decision making
`r vspace("10px")`
- ...

---

# Bayes theorem and its link with Bayesian inference

**Bayes' theorem**


- Provable from probability axioms
- Let $A$ and $B$ be events, then

$$ p(A|B) =\frac{p(A \cap B)}{p(B)} = \frac{ p(B|A) p(A) } {p(B)}$$

- If $A_i$ is a set of mutually exclusive and exhaustive events
(*i.e.* $A_i\cap A_j=\emptyset$, $p( \bigcup\limits_i A_i ) = \sum\limits_i p(A_i) = 1$), then

$$ p(A_i|B) = \frac{ p(B|A_i) p(A_i) } {p(B)} = \frac{ p(B|A_i) p(A_i) } {\sum\limits_j p(B|A_j) p(A_j) }$$

---

# An example: diagnostic tests

The latest COVID-19 test has shown to have 70% sensitivity and 99% specificity

In England, COVID prevalence is 6%; what
is the chance that a patient testing positive actually does have COVID-19?

`r vspace("10px")`

```{r,engine='tikz', echo=F, out.width="50%",opts=list(width="50%",title="INSERT TEXT HERE"),eval=TRUE}
\usetikzlibrary{shapes,arrows,decorations.pathreplacing,positioning,calc}
\begin{tikzpicture}
\draw(2,4.1) node[align=center,rectangle,rounded corners=2ex,draw=none,fill=none,font=\sffamily\fontsize{7}{7}\selectfont,minimum width=2.2cm](1){\color{red} Disease};
\draw(4.5,4.1) node[align=center,rectangle,rounded corners=2ex,draw=none,fill=none,font=\sffamily\fontsize{7}{7}\selectfont,minimum width=2.2cm](2){\color{green!70!black!90}No disease};

\draw(2,3.8) node[align=center,rectangle,rounded corners=2ex,draw=none,fill=none,font=\sffamily\fontsize{7}{7}\selectfont,minimum width=1.2cm](3){$p(A)=0.06$};

\draw(4.5,3.8) node[align=center,rectangle,rounded corners=2ex,draw=none,fill=none,font=\sffamily\fontsize{7}{7}\selectfont,minimum width=1.2cm](4){$1-P(A)=0.94$};

\draw(2.0,2) node[align=center,rectangle,rounded corners=2ex,draw=none,fill=none,font=\sffamily\fontsize{7}{7}\selectfont,minimum width=1.2cm](5){\textbf{$P(\overline{B})$}};

\draw(2.0,1.7) node[align=center,rectangle,rounded corners=2ex,draw=none,fill=none,font=\sffamily\fontsize{7}{7}\selectfont,minimum width=1.2cm](7){Negative Test};

\draw(4.5,2) node[align=center,rectangle,rounded corners=2ex,draw=none,fill=none,font=\sffamily\fontsize{7}{7}\selectfont,minimum width=1.2cm](6){$P(B)$};

\draw(4.5,1.75) node[align=center,rectangle,rounded corners=2ex,draw=none,fill=none,font=\sffamily\fontsize{7}{7}\selectfont,minimum width=1.2cm](8){Positive Test};

\draw [->,>=latex,shorten >=0pt,auto,node distance=0cm,ultra thin,color=red] (3.south) -- (5.north) node[midway,left,font=\fontsize{6}{7}\selectfont] {$0.3$}; 
\draw [->,>=latex,shorten >=0pt,auto,node distance=0cm,ultra thin,color=red] (3.300) -- (6.120) node[right,below,pos=.72,inner sep=5pt,font=\fontsize{6}{7}\selectfont] {$0.7$}; 
\draw [->,>=latex,shorten >=0pt,auto,node distance=0cm,ultra thin,color=green!70!black!90] (4.south) -- (6.north) node[midway,right,font=\fontsize{6}{7}\selectfont] {$0.01$}; 
\draw [->,>=latex,shorten >=0pt,auto,node distance=0cm,ultra thin,color=green!70!black!90] (4.240) -- (5.60) node[right,above,pos=.74,inner sep=5pt,font=\fontsize{6}{7}\selectfont] {$0.99$}; 
\end{tikzpicture}
```
--

We are interested in 

$$p(A|B) = \frac{ p(B|A) p(A) } {p(B|A) p(A) + p(B|\overline{A}) p(\overline{A})}=\frac{0.7 \times 0.06 } {0.7 \times  0.06 + 0.01 \times 0.94} = 0.81$$

---

# Comments

.pull-left[

- The disease prevalence can be thought of as a *prior* probability ( $p$ = 0.06)

`r vspace("20px")`

- Observing a positive result causes us to modify this probability to $p$ = 0.81. This is our *posterior* probability that patient is COVID-19 positive.

]

.pull-right[

`r include_fig("Bayes.png",width="80%",title="")`
]

--

- Bayes theorem applied to *observables* (as in diagnostic testing) is uncontroversial and established
- More controversial in general statistical analyses: *parameters* are unknown quantities, and prior distributions need to be specified $\rightarrow$ .red[Bayesian inference]

---

# Bayesian inference

Makes fundamental distinction between

- Observable quantities $y$, i.e. the data

- Unknown quantities $\theta$

  $\theta$ can be statistical parameters, missing data, mismeasured data...
`r vspace("10px")`
  $\rightarrow$ parameters are treated as random variables
`r vspace("10px")`
  $\rightarrow$ in the Bayesian framework, we make probability statements about model parameters


.content-box-green[
in the frequentist framework, parameters are fixed non-random quantities and the probability statements concerning the data]

`r vspace("10px")`
  
As with any statistical analysis, we start building a model which specifies $p(y \mid \theta)$

`r vspace("10px")`
This is the .red[likelihood], which relates all variables into a .red[full probability model]

---

# Bayesian inference [continued]

From a Bayesian point of view
- $\theta$ is unknown so should have a .red[probability distribution] reflecting
     our uncertainty about it before seeing the data
  
$\rightarrow$ need to specify a .red[prior distribution]  $p(\theta)$

- $y$ is known so we should condition on it

$\rightarrow$ use Bayes theorem to obtain conditional probability distribution for unobserved quantities of interest given the data:
\begin{align*}
p(\theta \mid y)&= \frac{ p(\theta, y)}{p(y)} = \frac{ p(\theta)\, p(y \mid \theta)}{\int  p(\theta)\,p(y \mid \theta)\,d\theta}\propto p(\theta)\,p(y \mid \theta)
\end{align*}

- This is the .red[posterior distribution]

`r vspace("10px")`

The prior distribution $p(\theta)$, expresses our uncertainty about $\theta$ .red[before] seeing the data

`r vspace("10px")`

The posterior distribution $p(\theta \mid y)$, expresses our uncertainty about $\theta$ .red[after] seeing the data

---

name: Components
  
`r vspace("250px")`

.myblue[.center[.huge[
**Bayesian computation**]]]

---

# How to obtain the posterior distribution?

- When the prior and posterior come from the same family of distributions
the prior is said to be **conjugate** to the likelihood $\rightarrow$ the posterior is a known distribution.

`r vspace("10px")`

- In real life it is (almost) impossible to use conjugacy so we need to resort to simulative approaches or approximations:

  - Monte Carlo methods
  `r vspace("10px")`

  - Markov Chain Monte Carlo
  `r vspace("10px")`

  - INLA

---
# Monte Carlo Simulation

- This approach is based on the idea that if you have a large random sample from a certain distribution, the statistics that you can calculate in this sample (mean, SD, percentiles...) will be very similar to the corresponding theoretical values in the distribution. 

`r vspace("10px")`

- If you have a complicated mathematical expression for a distribution and you cannot calculate algebraically important parameters, you could get the computer to generate a large random sample from such a distribution

`r vspace("10px")`

- By calculating the mean of that parameter in the sample you could estimate the mean in the original distribution with great precision 
---

# Monte Carlo approach to approximate log-odds

- We start with a Binomial likelihood 
$$y \mid \theta \sim \text{Binomial}(\theta, n)$$
combined with a 
$$\text{Beta}(a,b)$$

as prior for the probability of success $\theta$.

`r vspace("10px")`

- We are interested in the log-odds function of $\theta$ defined as 
$$\log \left(\frac{\theta}{1-\theta}\right)$$

- The integral 
$$\int_0^1 \log\left(\frac{\theta}{1-\theta}\right) p(\theta\mid y)\text{d}\theta$$ 
cannot be computed analytically; we resort to Monte Carlo approximation

---

# Example of MC: in practice

- We simulate $m$ independent values $\left\{\theta^{(1)},\ldots, \theta^{(m)}\right\}$ from the 
$$\text{Beta}(a_1=y+a,b_1=n-y+b)$$ posterior distribution using the property of conjugacy (Beta prior is conjugate to the Binomial likelihood). 
- We apply the log-odds transformation to each value obtaining the set of values
$$\left\{\log\left(\frac{\theta^{(1)}}{1-\theta^{(1)}}\right),\ldots,\log\left(\frac{\theta^{(m)}}{1-\theta^{(m)}}\right)\right\}$$
- Finally, we compute the sample mean 
$$\frac{\sum_{i=1}^m \log\left(\frac{\theta^{(i)}}{1-\theta^{(i)}}\right)}{m}$$
which is the Monte Carlo approximation to 
$$\log \left(\frac{\theta}{1-\theta}\right)$$

---

# Example of MC: R code

In `R`:

```{r eval=TRUE}
a <- 1
b <- 1
theta <- rbeta(1,a,b)
n <- 1000
y <- rbinom(1, size=n, p=theta)
a1 <- a + y
b1 <- n - y + b
```

--

- With this setting the exact posterior distribution of $\theta$ is 

$$\text{Beta}(a_1=a+y,b_1=n-y+b)$$

- To approximate the log-odds, we simulate $m=50000$ values from this Beta posterior distribution using the `rbeta` function.

```{r eval=TRUE}
sim <- rbeta(n=50000, shape1=a1, shape2=b1) 
logodds <- log(sim/(1-sim))
```

---

# Results and comparison with the theoretical distribution

The empirical distribution of the Monte Carlo sample is plotted below together with the exact posterior distribution of $\theta$.

.pull-left[
`r include_fig("MCexample.png",width="80%",title="")`
]

.pull-right[
`r include_fig("MCexampleLogOdds.png",width="80%",title="")`
]

---

# Why Markov Chain Monte Carlo?

- For all but trivial examples it will be difficult to draw an iid Monte Carlo
sample directly from the posterior distribution. 
  - This happens, for example, when the dimension of the parameter vector $\bm{\theta}$ is high  
  - Also to use MC methods we must have a known form for the posterior distribution 

--

`r vspace("10px")`

- Alternatively *correlated* values (via MCMC) can be (more easily) drawn to approximate the posterior distribution of the parameter of interest

- Instead of simulating independent values from the posterior distribution we draw a sample by running a Markov chain whose stationary distribution is the posterior density $p(\theta\mid y)$

--

`r vspace("10px")`

- A sequence of values $\left\{\theta^{(1)},\ldots, \theta^{(m)}\right\}$  generated from a Markov chain that has reached its stationary distribution (i.e. has converged) can be considered as an approximation to the posterior distribution and can be used to compute all the summaries of interest.

--

`r vspace("10px")`

- MCMC methods are very general and can effectively be applied to `any` model

- Even if **in theory**, MCMC can provide (nearly) exact inference, given perfect convergence and MC error $\rightarrow 0$, in practice, this has to be balanced with model complexity and running time 

- This is an issue particularly for problems characterised by large data or very complex structure (e.g. hierarchical models)

---

# MCMC: Gibbs sampling

The .red[**Gibbs sampling**] (GS) is one of the most popular schemes for MCMC. Consider the case of a generic $J$ dimensional parameter set $(\theta_1,\theta_2,\ldots,\theta_J)$:


1. Select a set of initial values $\color{blue}{(\theta^{(0)}_1,\theta^{(0)}_2,\ldots,\theta^{(0)}_J)}$

2. Sample $\color{blue}{\theta^{(1)}_1}$ from the conditional distribution $\color{blue}{ p(\theta_1\mid \theta^{(0)}_2,\theta^{(0)}_3,\ldots,\theta^{(0)}_J,y)}$;
Sample $\color{blue}{\theta^{(1)}_2}$ from the conditional distribution $\color{blue}{p(\theta_2\mid \theta^{(1)}_1,\theta^{(0)}_3,\ldots,\theta^{(0)}_J,y)}$;
...;
Sample $\color{blue}{\theta^{(1)}_J}$ from the conditional distribution $\color{blue}{p(\theta_J\mid \theta^{(1)}_1,\theta^{(1)}_2,\ldots,\theta^{(1)}_{J-1},y)}$

--

3. Repeat step 2. for $S$ times until convergence is reached to the target distribution $\color{blue}{p(\bm\theta\mid y)}$

4. Use the sample from the target distribution to compute all relevant statistics: (posterior) mean, variance, credibility intervals, etc.

If the *full conditionals* are not readily available, they need to be estimated (eg via Metropolis-Hastings) before applying the GS 

Easy references for MCMC are 

- `r Citet(bibfile,"blangiardo2015spatial")`, Chapter 4

- `r Citet(bibfile, "johnson2022bayes")`, Chapters 6-7

---

# MCMC: convergence

`r include_fig("NormGibbs1.jpeg",width="55%")`

---

count: false

# MCMC: convergence

`r include_fig("NormGibbs2.jpeg",width="55%")`

---

count: false

# MCMC: convergence

`r include_fig("NormGibbs3.jpeg",width="55%")`

---

# MCMC: pros & cons

- `Standard` MCMC sampler are generally easy-ish to program and are in fact implemented in readily available software

- MCMC methods are flexible and able to deal with virtually any type of data and model, but they  involve computationally- and time- intensive simulations to obtain the posterior distribution for the parameters. For this reason the complexity of the model and the database dimension often remain fundamental issues. 
--

- The INLA algorithm proposed by `r Citep(bibfile,"INLA:09")` is a .red[*deterministic*] algorithm for Bayesian inference and it represents an alternative to MCMC which is instead a simulation based algorithm.

--

- The INLA algorithm is designed for the class of .red[*latent Gaussian models*] and compared to MCMC it provides (as) accurate results in a shorter time.

--

- INLA has become very popular amongst statisticians and applied researchers  and in the past few years the number of papers reporting usage and extensions of the INLA method has increased considerably.

---

# References

```{r refs, echo=FALSE, results="asis"}
PrintBibliography(bibfile,.opts=list(max.names=3))
```
