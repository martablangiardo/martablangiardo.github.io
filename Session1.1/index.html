<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Session 1.1: Bayesian thinking</title>
    <meta charset="utf-8" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/panelset/panelset.css" rel="stylesheet" />
    <script src="libs/panelset/panelset.js"></script>
    <!-- (Re)Defines a bunch of LaTeX commands that can then be used directly in the .Rmd file as '\command{...}' -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      TeX: {
        /* This enables color macros */
        extensions: ["color.js"],
        Macros: {
          /* Probability & mathematical symbols */
          Pr: "{\\style{font-family:inherit; font-size: 110%;}{\\text{Pr}}}",
          exp: "{\\style{font-family:inherit; font-size: 105%;}{\\text{exp}}}",
          log: "{\\style{font-family:inherit; font-size: 105%;}{\\text{log}}}",
          ln: "{\\style{font-family:inherit; font-size: 105%;}{\\text{ln}}}",
          logit: "{\\style{font-family:inherit; font-size: 100%;}{\\text{logit}}}",
          HR: "{\\style{font-family:inherit; font-size: 105%;}{\\text{HR}}}",
          OR: "{\\style{font-family:inherit; font-size: 105%;}{\\text{OR}}}",
          E: "{\\style{font-family:inherit; font-size: 105%;}{\\text{E}}}",
          Var: "{\\style{font-family:inherit; font-size: 105%;}{\\text{Var}}}",
          Cov: "{\\style{font-family:inherit; font-size: 105%;}{\\text{Cov}}}",
          Corr: "{\\style{font-family:inherit; font-size: 105%;}{\\text{Corr}}}",
          DIC: "{\\style{font-family:inherit; font-size: 105%;}{\\text{DIC}}}",
          se: "{\\style{font-family:inherit; font-size: 100%;}{\\text{se}}}",
          sd: "{\\style{font-family:inherit; font-size: 100%;}{\\text{sd}}}",
          kld: "{\\style{font-family:inherit; font-size: 100%;}{\\text{kld}}}",
          /* Distributions */
          dnorm: "{\\style{font-family:inherit;}{\\text{Normal}}}",
          dt: "{\\style{font-family:inherit;}{\\text{t}}}",
          ddirch: "{\\style{font-family:inherit;}{\\text{Dirichlet}}}",
          dmulti: "{\\style{font-family:inherit;}{\\text{Multinomial}}}",
          dbeta: "{\\style{font-family:inherit;}{\\text{Beta}}}",
          dgamma: "{\\style{font-family:inherit;}{\\text{Gamma}}}",
          dbern: "{\\style{font-family:inherit;}{\\text{Bernoulli}}}",
          dbin: "{\\style{font-family:inherit;}{\\text{Binomial}}}",
          dpois: "{\\style{font-family:inherit;}{\\text{Poisson}}}",
          dweib: "{\\style{font-family:inherit;}{\\text{Weibull}}}",
          dexp: "{\\style{font-family:inherit;}{\\text{Exponential}}}",
          dlnorm: "{\\style{font-family:inherit;}{\\text{logNormal}}}",
          dunif: "{\\style{font-family:inherit;}{\\text{Uniform}}}",
          /* LaTeX formatting */
          bm: ["{\\boldsymbol #1}",1],
          /* These create macros to typeset numbers in maths with the basic font */
          0: "{\\style{font-family:inherit; font-size: 105%;}{\\text{0}}}",
          1: "{\\style{font-family:inherit; font-size: 105%;}{\\text{1}}}",
          2: "{\\style{font-family:inherit; font-size: 105%;}{\\text{2}}}",
          3: "{\\style{font-family:inherit; font-size: 105%;}{\\text{3}}}",
          4: "{\\style{font-family:inherit; font-size: 105%;}{\\text{4}}}",
          5: "{\\style{font-family:inherit; font-size: 105%;}{\\text{5}}}",
          6: "{\\style{font-family:inherit; font-size: 105%;}{\\text{6}}}",
          7: "{\\style{font-family:inherit; font-size: 105%;}{\\text{7}}}",
          8: "{\\style{font-family:inherit; font-size: 105%;}{\\text{8}}}",
          9: "{\\style{font-family:inherit; font-size: 105%;}{\\text{9}}}",
          /* Health economics quantities */
          icer: "{\\style{font-family:inherit; font-size: 100%;}{\\text{ICER}}}",
          nb: "{\\style{font-family:inherit; font-size: 100%;}{\\text{NB}}}",
          ol: "{\\style{font-family:inherit; font-size: 100%;}{\\text{OL}}}",
          ceac: "{\\style{font-family:inherit; font-size: 100%;}{\\text{CEAC}}}",
          evpi: "{\\style{font-family:inherit; font-size: 100%;}{\\text{EVPI}}}",
          evppi: "{\\style{font-family:inherit; font-size: 100%;}{\\text{EVPPI}}}",
          evsi: "{\\style{font-family:inherit; font-size: 100%;}{\\text{EVSI}}}"
        }
      }
    });
    </script>
    <link rel="stylesheet" href="assets/beamer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">




class: title-slide

# Session 1.1: Bayesian thinking&lt;span style="display:block; margin-top: 10px ;"&gt;&lt;/span&gt; 

## 

###     

### Spatial and Spatio-Temporal Bayesian Models with `R-INLA`, University of São Paulo 

&lt;!-- Can also separate the various components of the extra argument 'params', eg as in 
### Spatial and Spatio-Temporal Bayesian Models with `R-INLA`, University of São Paulo, 26 September 2022, Spatial and Spatio-Temporal Bayesian Models with `R-INLA`
--&gt;

26 September 2022

---

layout: true  

.my-footer[ 
.alignleft[ 
&amp;nbsp; &amp;copy; Marta Blangiardo | Monica Pirani 
]
.aligncenter[
Spatial and Spatio-Temporal Bayesian Models with `R-INLA` 
]
.alignright[
Spatial and Spatio-Temporal Bayesian Models with `R-INLA`, 26 Sep 2022 
]
] 


&lt;style&gt;
pre {
  overflow-x: auto;
}
pre code {
  word-wrap: normal;
  white-space: pre;
}
&lt;/style&gt;


---

# Learning objectives

After this lecture you should be able to 
&lt;span style="display:block; margin-top: 40px ;"&gt;&lt;/span&gt;
- Introduce Bayesian way of thinking     
&lt;span style="display:block; margin-top: 40px ;"&gt;&lt;/span&gt;
- Present Bayes theorem and Bayesian inference   
&lt;span style="display:block; margin-top: 40px ;"&gt;&lt;/span&gt;
- Describe computational methods commonly used to perform Bayesian inference     
&lt;span style="display:block; margin-top: 40px ;"&gt;&lt;/span&gt;

The topics treated in this lecture are presented in Chapter 3-4  of the book **Spatial and Spatio-Temporal Bayesian models with R-INLA**.

---

# Outline 

&lt;span style="display:block; margin-top: 30px ;"&gt;&lt;/span&gt;

1\. [Why Bayesian](#WhyBayesian)

&lt;span style="display:block; margin-top: 30px ;"&gt;&lt;/span&gt;

2\. [Components of a Bayesian analysis](#Components)

&lt;span style="display:block; margin-top: 30px ;"&gt;&lt;/span&gt;

3\. [Bayes Theorem](#BayesTheo)

&lt;span style="display:block; margin-top: 30px ;"&gt;&lt;/span&gt;

4\. [Bayesian computing](#BayComp)

---

name: WhyBayesian
  
&lt;span style="display:block; margin-top: 250px ;"&gt;&lt;/span&gt;

.myblue[.center[.huge[
**Why Bayesian**]]]

---

# Statistics - The  'Big Picture'

&lt;center&gt;&lt;img src=./img/BigPicture0.png width='60%' title=''&gt;&lt;/center&gt;

---

# Statistics - The  'Big Picture'


&lt;center&gt;&lt;img src=./img/BigPicture.png width='60%' title=''&gt;&lt;/center&gt;

&lt;span style="display:block; margin-top: -100px ;"&gt;&lt;/span&gt;

- Several different ways of formulating statistical models and computing inferences from these models and data
- Can be grouped into two broad approaches:
  - Frequentist
  - Bayesian

---

# Everyday thought process

- At the start of the football season I formed a view about the chance that the team I support will be relegated (**PRIOR**), based on

  - performance last season
  
  - summer transfers
  - etc.

--

&lt;span style="display:block; margin-top: 20px ;"&gt;&lt;/span&gt;

- The first match is played

&lt;span style="display:block; margin-top: 20px ;"&gt;&lt;/span&gt;

- .red[Now I have some information from current season (**DATA**)]

--

&lt;span style="display:block; margin-top: 20px ;"&gt;&lt;/span&gt;

- I re-assesses the probability of relegation upwards, because
  - they lose
  - their main striker limps off

&lt;span style="display:block; margin-top: 20px ;"&gt;&lt;/span&gt;
  
- .red[Prior view updated with data to give **POSTERIOR** view]


---

# Thought process of a physician

- A patient presents with a set of symptoms, concerned that they might have a certain disease

&lt;span style="display:block; margin-top: 10px ;"&gt;&lt;/span&gt;

- The physician assesses the chance that the patient has this disease, based on

  - symptoms
  - family history
  - alternative explanations of symptoms
  - prevalence of disease

&lt;span style="display:block; margin-top: 10px ;"&gt;&lt;/span&gt;

- The physician sends the patient for a diagnostic test
&lt;span style="display:block; margin-top: 10px ;"&gt;&lt;/span&gt;
- The physician re-assesses the chance that the patient has this disease, taking account of

  - results of diagnostic test
  - reliability of diagnostic test
&lt;span style="display:block; margin-top: 10px ;"&gt;&lt;/span&gt;
- The physician may send the patient for further diagnostic tests

---

# Bayesian thinking

&lt;center&gt;&lt;img src=./img/Thinking1.png width='80%' title=''&gt;&lt;/center&gt;

---

# Bayesian thinking

&lt;center&gt;&lt;img src=./img/Thinking2.png width='80%' title=''&gt;&lt;/center&gt;

---

# Why Bayesian methods?

- Bayesian methods have been widely applied in many areas:
&lt;span style="display:block; margin-top: 10px ;"&gt;&lt;/span&gt;
  - medicine / epidemiology
  - genetics
  - ecology
  - environmental sciences
  - social and political sciences
  - finance
  - archaeology
  - .....
&lt;span style="display:block; margin-top: 10px ;"&gt;&lt;/span&gt;
- Motivations for adopting Bayesian approach vary:
&lt;span style="display:block; margin-top: 10px ;"&gt;&lt;/span&gt;
  - natural and coherent way of thinking about science and learning
  - pragmatic choice that is suitable for the problem in hand

---

# Example

A clinical trial is carried out to collect evidence about an unknown *treatment effect*

&lt;span style="display:block; margin-top: 20px ;"&gt;&lt;/span&gt;

.red[Conventional analysis]

- p-value for `\(H_0\)`: treatment effect is zero
&lt;span style="display:block; margin-top: 10px ;"&gt;&lt;/span&gt;

- Point estimate and CI as summaries of  size of treatment effect

**Aim is to learn what this trial tells us about the treatment effect**

--

&lt;span style="display:block; margin-top: 20px ;"&gt;&lt;/span&gt;

.red[Bayesian analysis]

- Inference is based on probability statements summarising the posterior distribution of the treatment effect

Asks: **how should this trial change our opinion about the treatment effect?**

---

name: Components
  
&lt;span style="display:block; margin-top: 250px ;"&gt;&lt;/span&gt;

.myblue[.center[.huge[
**Components of a Bayesian analysis**]]]

---

# Components of a Bayesian analysis

A clinical trial is carried out to collect evidence about an
unknown *treatment effect*. The Bayesian analyst needs to explicitly state
&lt;span style="display:block; margin-top: 10px ;"&gt;&lt;/span&gt;

- a reasonable opinion concerning the plausibility of different values of the treatment effect *excluding* the evidence from the trial (the .red[prior distribution])
&lt;span style="display:block; margin-top: 10px ;"&gt;&lt;/span&gt;

- the support for different values of the treatment effect based *solely* on data from the
   trial (the .red[likelihood]),

&lt;span style="display:block; margin-top: 10px ;"&gt;&lt;/span&gt;

and to combine these two sources to produce

- a final opinion about the treatment effect (the .red[posterior distribution])

--

&lt;span style="display:block; margin-top: 20px ;"&gt;&lt;/span&gt;


The final combination is done using **Bayes theorem** (and only simple rules of probability), which essentially weights the likelihood from the trial with the relative plausibilities defined by the prior distribution

&lt;span style="display:block; margin-top: 20px ;"&gt;&lt;/span&gt;


.content-box-green[
One can view the Bayesian approach as a formalisation of the process
of learning from experience]

---

# Bayesian inference: the posterior distribution

Posterior distribution forms basis for all inference --- can
be summarised to provide
&lt;span style="display:block; margin-top: 10px ;"&gt;&lt;/span&gt;
- point and interval estimates of Quantities of Interest (QOI), e.g. treatment effect, small area estimates, ...
&lt;span style="display:block; margin-top: 10px ;"&gt;&lt;/span&gt;
- point and interval estimates of any function of the parameters
&lt;span style="display:block; margin-top: 10px ;"&gt;&lt;/span&gt;
- probability that QOI (e.g. treatment effect) exceeds a critical threshold
&lt;span style="display:block; margin-top: 10px ;"&gt;&lt;/span&gt;
- prediction of QOI in a new unit
&lt;span style="display:block; margin-top: 10px ;"&gt;&lt;/span&gt;
- prior information for future experiments, trials, surveys, ...
&lt;span style="display:block; margin-top: 10px ;"&gt;&lt;/span&gt;
- inputs for decision making
&lt;span style="display:block; margin-top: 10px ;"&gt;&lt;/span&gt;
- ...

---

# Bayes theorem and its link with Bayesian inference

**Bayes' theorem**


- Provable from probability axioms
- Let `\(A\)` and `\(B\)` be events, then

$$ p(A|B) =\frac{p(A \cap B)}{p(B)} = \frac{ p(B|A) p(A) } {p(B)}$$

- If `\(A_i\)` is a set of mutually exclusive and exhaustive events
(*i.e.* `\(A_i\cap A_j=\emptyset\)`, `\(p( \bigcup\limits_i A_i ) = \sum\limits_i p(A_i) = 1\)`), then

$$ p(A_i|B) = \frac{ p(B|A_i) p(A_i) } {p(B)} = \frac{ p(B|A_i) p(A_i) } {\sum\limits_j p(B|A_j) p(A_j) }$$

---

# An example: diagnostic tests

The latest COVID-19 test has shown to have 70% sensitivity and 99% specificity

In England, COVID prevalence is 6%; what
is the chance that a patient testing positive actually does have COVID-19?

&lt;span style="display:block; margin-top: 10px ;"&gt;&lt;/span&gt;

&lt;img src="./img/unnamed-chunk-3-1.png" style="display: block; margin: auto;" width="50%" title="INSERT TEXT HERE"&gt;
--

We are interested in 

`$$p(A|B) = \frac{ p(B|A) p(A) } {p(B|A) p(A) + p(B|\overline{A}) p(\overline{A})}=\frac{0.7 \times 0.06 } {0.7 \times  0.06 + 0.01 \times 0.94} = 0.81$$`

---

# Comments

.pull-left[

- The disease prevalence can be thought of as a *prior* probability ( `\(p\)` = 0.06)

&lt;span style="display:block; margin-top: 20px ;"&gt;&lt;/span&gt;

- Observing a positive result causes us to modify this probability to `\(p\)` = 0.81. This is our *posterior* probability that patient is COVID-19 positive.

]

.pull-right[

&lt;center&gt;&lt;img src=./img/Bayes.png width='80%' title=''&gt;&lt;/center&gt;
]

--

- Bayes theorem applied to *observables* (as in diagnostic testing) is uncontroversial and established
- More controversial in general statistical analyses: *parameters* are unknown quantities, and prior distributions need to be specified `\(\rightarrow\)` .red[Bayesian inference]

---

# Bayesian inference

Makes fundamental distinction between

- Observable quantities `\(y\)`, i.e. the data

- Unknown quantities `\(\theta\)`

  `\(\theta\)` can be statistical parameters, missing data, mismeasured data...
&lt;span style="display:block; margin-top: 10px ;"&gt;&lt;/span&gt;
  `\(\rightarrow\)` parameters are treated as random variables
&lt;span style="display:block; margin-top: 10px ;"&gt;&lt;/span&gt;
  `\(\rightarrow\)` in the Bayesian framework, we make probability statements about model parameters


.content-box-green[
in the frequentist framework, parameters are fixed non-random quantities and the probability statements concerning the data]

&lt;span style="display:block; margin-top: 10px ;"&gt;&lt;/span&gt;
  
As with any statistical analysis, we start building a model which specifies `\(p(y \mid \theta)\)`

&lt;span style="display:block; margin-top: 10px ;"&gt;&lt;/span&gt;
This is the .red[likelihood], which relates all variables into a .red[full probability model]

---

# Bayesian inference [continued]

From a Bayesian point of view
- `\(\theta\)` is unknown so should have a .red[probability distribution] reflecting
     our uncertainty about it before seeing the data
  
`\(\rightarrow\)` need to specify a .red[prior distribution]  `\(p(\theta)\)`

- `\(y\)` is known so we should condition on it

`\(\rightarrow\)` use Bayes theorem to obtain conditional probability distribution for unobserved quantities of interest given the data:
`\begin{align*}
p(\theta \mid y)&amp;= \frac{ p(\theta, y)}{p(y)} = \frac{ p(\theta)\, p(y \mid \theta)}{\int  p(\theta)\,p(y \mid \theta)\,d\theta}\propto p(\theta)\,p(y \mid \theta)
\end{align*}`

- This is the .red[posterior distribution]

&lt;span style="display:block; margin-top: 10px ;"&gt;&lt;/span&gt;

The prior distribution `\(p(\theta)\)`, expresses our uncertainty about `\(\theta\)` .red[before] seeing the data

&lt;span style="display:block; margin-top: 10px ;"&gt;&lt;/span&gt;

The posterior distribution `\(p(\theta \mid y)\)`, expresses our uncertainty about `\(\theta\)` .red[after] seeing the data

---

name: Components
  
&lt;span style="display:block; margin-top: 250px ;"&gt;&lt;/span&gt;

.myblue[.center[.huge[
**Bayesian computation**]]]

---

# How to obtain the posterior distribution?

- When the prior and posterior come from the same family of distributions
the prior is said to be **conjugate** to the likelihood `\(\rightarrow\)` the posterior is a known distribution.

&lt;span style="display:block; margin-top: 10px ;"&gt;&lt;/span&gt;

- In real life it is (almost) impossible to use conjugacy so we need to resort to simulative approaches or approximations:

  - Monte Carlo methods
  &lt;span style="display:block; margin-top: 10px ;"&gt;&lt;/span&gt;

  - Markov Chain Monte Carlo
  &lt;span style="display:block; margin-top: 10px ;"&gt;&lt;/span&gt;

  - INLA

---
# Monte Carlo Simulation

- This approach is based on the idea that if you have a large random sample from a certain distribution, the statistics that you can calculate in this sample (mean, SD, percentiles...) will be very similar to the corresponding theoretical values in the distribution. 

&lt;span style="display:block; margin-top: 10px ;"&gt;&lt;/span&gt;

- If you have a complicated mathematical expression for a distribution and you cannot calculate algebraically important parameters, you could get the computer to generate a large random sample from such a distribution

&lt;span style="display:block; margin-top: 10px ;"&gt;&lt;/span&gt;

- By calculating the mean of that parameter in the sample you could estimate the mean in the original distribution with great precision 
---

# Monte Carlo approach to approximate log-odds

- We start with a Binomial likelihood 
`$$y \mid \theta \sim \text{Binomial}(\theta, n)$$`
combined with a 
`$$\text{Beta}(a,b)$$`

as prior for the probability of success `\(\theta\)`.

&lt;span style="display:block; margin-top: 10px ;"&gt;&lt;/span&gt;

- We are interested in the log-odds function of `\(\theta\)` defined as 
`$$\log \left(\frac{\theta}{1-\theta}\right)$$`

- The integral 
`$$\int_0^1 \log\left(\frac{\theta}{1-\theta}\right) p(\theta\mid y)\text{d}\theta$$` 
cannot be computed analytically; we resort to Monte Carlo approximation

---

# Example of MC: in practice

- We simulate `\(m\)` independent values `\(\left\{\theta^{(1)},\ldots, \theta^{(m)}\right\}\)` from the 
`$$\text{Beta}(a_1=y+a,b_1=n-y+b)$$` posterior distribution using the property of conjugacy (Beta prior is conjugate to the Binomial likelihood). 
- We apply the log-odds transformation to each value obtaining the set of values
`$$\left\{\log\left(\frac{\theta^{(1)}}{1-\theta^{(1)}}\right),\ldots,\log\left(\frac{\theta^{(m)}}{1-\theta^{(m)}}\right)\right\}$$`
- Finally, we compute the sample mean 
`$$\frac{\sum_{i=1}^m \log\left(\frac{\theta^{(i)}}{1-\theta^{(i)}}\right)}{m}$$`
which is the Monte Carlo approximation to 
`$$\log \left(\frac{\theta}{1-\theta}\right)$$`

---

# Example of MC: R code

In `R`:


```r
&gt; a &lt;- 1
&gt; b &lt;- 1
&gt; theta &lt;- rbeta(1,a,b)
&gt; n &lt;- 1000
&gt; y &lt;- rbinom(1, size=n, p=theta)
&gt; a1 &lt;- a + y
&gt; b1 &lt;- n - y + b
```

--

- With this setting the exact posterior distribution of `\(\theta\)` is 

`$$\text{Beta}(a_1=a+y,b_1=n-y+b)$$`

- To approximate the log-odds, we simulate `\(m=50000\)` values from this Beta posterior distribution using the `rbeta` function.


```r
&gt; sim &lt;- rbeta(n=50000, shape1=a1, shape2=b1) 
&gt; logodds &lt;- log(sim/(1-sim))
```

---

# Results and comparison with the theoretical distribution

The empirical distribution of the Monte Carlo sample is plotted below together with the exact posterior distribution of `\(\theta\)`.

.pull-left[
&lt;center&gt;&lt;img src=./img/MCexample.png width='80%' title=''&gt;&lt;/center&gt;
]

.pull-right[
&lt;center&gt;&lt;img src=./img/MCexampleLogOdds.png width='80%' title=''&gt;&lt;/center&gt;
]

---

# Why Markov Chain Monte Carlo?

- For all but trivial examples it will be difficult to draw an iid Monte Carlo
sample directly from the posterior distribution. 
  - This happens, for example, when the dimension of the parameter vector `\(\bm{\theta}\)` is high  
  - Also to use MC methods we must have a known form for the posterior distribution 

--

&lt;span style="display:block; margin-top: 10px ;"&gt;&lt;/span&gt;

- Alternatively *correlated* values (via MCMC) can be (more easily) drawn to approximate the posterior distribution of the parameter of interest

- Instead of simulating independent values from the posterior distribution we draw a sample by running a Markov chain whose stationary distribution is the posterior density `\(p(\theta\mid y)\)`

--

&lt;span style="display:block; margin-top: 10px ;"&gt;&lt;/span&gt;

- A sequence of values `\(\left\{\theta^{(1)},\ldots, \theta^{(m)}\right\}\)`  generated from a Markov chain that has reached its stationary distribution (i.e. has converged) can be considered as an approximation to the posterior distribution and can be used to compute all the summaries of interest.

--

&lt;span style="display:block; margin-top: 10px ;"&gt;&lt;/span&gt;

- MCMC methods are very general and can effectively be applied to `any` model

- Even if **in theory**, MCMC can provide (nearly) exact inference, given perfect convergence and MC error `\(\rightarrow 0\)`, in practice, this has to be balanced with model complexity and running time 

- This is an issue particularly for problems characterised by large data or very complex structure (e.g. hierarchical models)

---

# MCMC: Gibbs sampling

The .red[**Gibbs sampling**] (GS) is one of the most popular schemes for MCMC. Consider the case of a generic `\(J\)` dimensional parameter set `\((\theta_1,\theta_2,\ldots,\theta_J)\)`:


1. Select a set of initial values `\(\color{blue}{(\theta^{(0)}_1,\theta^{(0)}_2,\ldots,\theta^{(0)}_J)}\)`

2. Sample `\(\color{blue}{\theta^{(1)}_1}\)` from the conditional distribution `\(\color{blue}{ p(\theta_1\mid \theta^{(0)}_2,\theta^{(0)}_3,\ldots,\theta^{(0)}_J,y)}\)`;
Sample `\(\color{blue}{\theta^{(1)}_2}\)` from the conditional distribution `\(\color{blue}{p(\theta_2\mid \theta^{(1)}_1,\theta^{(0)}_3,\ldots,\theta^{(0)}_J,y)}\)`;
...;
Sample `\(\color{blue}{\theta^{(1)}_J}\)` from the conditional distribution `\(\color{blue}{p(\theta_J\mid \theta^{(1)}_1,\theta^{(1)}_2,\ldots,\theta^{(1)}_{J-1},y)}\)`

--

3. Repeat step 2. for `\(S\)` times until convergence is reached to the target distribution `\(\color{blue}{p(\bm\theta\mid y)}\)`

4. Use the sample from the target distribution to compute all relevant statistics: (posterior) mean, variance, credibility intervals, etc.

If the *full conditionals* are not readily available, they need to be estimated (eg via Metropolis-Hastings) before applying the GS 

Easy references for MCMC are 

- Blangiardo and Cameletti (2015), Chapter 4

- Johnson, Ott, and Dogucu (2022), Chapters 6-7

---

# MCMC: convergence

&lt;center&gt;&lt;img src=./img/NormGibbs1.jpeg width='55%' title='INCLUDE TEXT HERE'&gt;&lt;/center&gt;

---

count: false

# MCMC: convergence

&lt;center&gt;&lt;img src=./img/NormGibbs2.jpeg width='55%' title='INCLUDE TEXT HERE'&gt;&lt;/center&gt;

---

count: false

# MCMC: convergence

&lt;center&gt;&lt;img src=./img/NormGibbs3.jpeg width='55%' title='INCLUDE TEXT HERE'&gt;&lt;/center&gt;

---

# MCMC: pros &amp; cons

- `Standard` MCMC sampler are generally easy-ish to program and are in fact implemented in readily available software

- MCMC methods are flexible and able to deal with virtually any type of data and model, but they  involve computationally- and time- intensive simulations to obtain the posterior distribution for the parameters. For this reason the complexity of the model and the database dimension often remain fundamental issues. 
--

- The INLA algorithm proposed by (Rue, Martino, and Chopin, 2009) is a .red[*deterministic*] algorithm for Bayesian inference and it represents an alternative to MCMC which is instead a simulation based algorithm.

--

- The INLA algorithm is designed for the class of .red[*latent Gaussian models*] and compared to MCMC it provides (as) accurate results in a shorter time.

--

- INLA has become very popular amongst statisticians and applied researchers  and in the past few years the number of papers reporting usage and extensions of the INLA method has increased considerably.

---

# References

Blangiardo, M. and M. Cameletti (2015). _Spatial and spatio-temporal
Bayesian models with R-INLA_. John Wiley &amp; Sons.

Johnson, A. A., M. Q. Ott, and M. Dogucu (2022). _Bayes Rules!: An
Introduction to Applied Bayesian Modeling_. CRC Press.

Rue, H., S. Martino, and N. Chopin (2009). "Approximate Bayesian
inference for latent Gaussian model by using integrated nested Laplace
approximations (with discussion)". In: _J. R. Statist. Soc. B_ 71, pp.
319-392.
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="https://platform.twitter.com/widgets.js"></script>
<script>var slideshow = remark.create({
"navigation": {
"scroll": false
},
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>
<style>
.logo {
  background-image: url("assets/MRC-Centre-Logo.png");
  background-size: 15% 10%;
  background-repeat: no-repeat;
  position: absolute;
  top:  0.25%; /* 1.135em */
  left: 85%;
  width: 100%;
  height: 100%;
  z-index: 0;
}
</style>

<script>
document
  .querySelectorAll(
    '.remark-slide-content' +
    ':not(.title-slide)' +
    // add additional classes to exclude here, e.g.
    // ':not(.inverse)' +
    ':not(.hide-logo)' +
    ':not(.thankyou-michelle)' +
    ':not(.thankyou-barney)'
  )
  .forEach(el => {
    el.innerHTML += '<div class="logo"></div>';
  });
</script>


<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
