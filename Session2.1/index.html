<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Session 2.1: Bayesian inference</title>
    <meta charset="utf-8" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/panelset/panelset.css" rel="stylesheet" />
    <script src="libs/panelset/panelset.js"></script>
    <script src="libs/kePrint/kePrint.js"></script>
    <link href="libs/lightable/lightable.css" rel="stylesheet" />
    <!-- (Re)Defines a bunch of LaTeX commands that can then be used directly in the .Rmd file as '\command{...}' -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      TeX: {
        /* This enables color macros */
        extensions: ["color.js"],
        Macros: {
          /* Probability & mathematical symbols */
          Pr: "{\\style{font-family:inherit; font-size: 110%;}{\\text{Pr}}}",
          exp: "{\\style{font-family:inherit; font-size: 105%;}{\\text{exp}}}",
          log: "{\\style{font-family:inherit; font-size: 105%;}{\\text{log}}}",
          ln: "{\\style{font-family:inherit; font-size: 105%;}{\\text{ln}}}",
          logit: "{\\style{font-family:inherit; font-size: 100%;}{\\text{logit}}}",
          HR: "{\\style{font-family:inherit; font-size: 105%;}{\\text{HR}}}",
          OR: "{\\style{font-family:inherit; font-size: 105%;}{\\text{OR}}}",
          E: "{\\style{font-family:inherit; font-size: 105%;}{\\text{E}}}",
          Var: "{\\style{font-family:inherit; font-size: 105%;}{\\text{Var}}}",
          Cov: "{\\style{font-family:inherit; font-size: 105%;}{\\text{Cov}}}",
          Corr: "{\\style{font-family:inherit; font-size: 105%;}{\\text{Corr}}}",
          DIC: "{\\style{font-family:inherit; font-size: 105%;}{\\text{DIC}}}",
          se: "{\\style{font-family:inherit; font-size: 100%;}{\\text{se}}}",
          sd: "{\\style{font-family:inherit; font-size: 100%;}{\\text{sd}}}",
          kld: "{\\style{font-family:inherit; font-size: 100%;}{\\text{kld}}}",
          /* Distributions */
          dnorm: "{\\style{font-family:inherit;}{\\text{Normal}}}",
          dt: "{\\style{font-family:inherit;}{\\text{t}}}",
          ddirch: "{\\style{font-family:inherit;}{\\text{Dirichlet}}}",
          dmulti: "{\\style{font-family:inherit;}{\\text{Multinomial}}}",
          dbeta: "{\\style{font-family:inherit;}{\\text{Beta}}}",
          dgamma: "{\\style{font-family:inherit;}{\\text{Gamma}}}",
          dbern: "{\\style{font-family:inherit;}{\\text{Bernoulli}}}",
          dbin: "{\\style{font-family:inherit;}{\\text{Binomial}}}",
          dpois: "{\\style{font-family:inherit;}{\\text{Poisson}}}",
          dweib: "{\\style{font-family:inherit;}{\\text{Weibull}}}",
          dexp: "{\\style{font-family:inherit;}{\\text{Exponential}}}",
          dlnorm: "{\\style{font-family:inherit;}{\\text{logNormal}}}",
          dunif: "{\\style{font-family:inherit;}{\\text{Uniform}}}",
          /* LaTeX formatting */
          bm: ["{\\boldsymbol #1}",1],
          /* These create macros to typeset numbers in maths with the basic font */
          0: "{\\style{font-family:inherit; font-size: 105%;}{\\text{0}}}",
          1: "{\\style{font-family:inherit; font-size: 105%;}{\\text{1}}}",
          2: "{\\style{font-family:inherit; font-size: 105%;}{\\text{2}}}",
          3: "{\\style{font-family:inherit; font-size: 105%;}{\\text{3}}}",
          4: "{\\style{font-family:inherit; font-size: 105%;}{\\text{4}}}",
          5: "{\\style{font-family:inherit; font-size: 105%;}{\\text{5}}}",
          6: "{\\style{font-family:inherit; font-size: 105%;}{\\text{6}}}",
          7: "{\\style{font-family:inherit; font-size: 105%;}{\\text{7}}}",
          8: "{\\style{font-family:inherit; font-size: 105%;}{\\text{8}}}",
          9: "{\\style{font-family:inherit; font-size: 105%;}{\\text{9}}}",
          /* Health economics quantities */
          icer: "{\\style{font-family:inherit; font-size: 100%;}{\\text{ICER}}}",
          nb: "{\\style{font-family:inherit; font-size: 100%;}{\\text{NB}}}",
          ol: "{\\style{font-family:inherit; font-size: 100%;}{\\text{OL}}}",
          ceac: "{\\style{font-family:inherit; font-size: 100%;}{\\text{CEAC}}}",
          evpi: "{\\style{font-family:inherit; font-size: 100%;}{\\text{EVPI}}}",
          evppi: "{\\style{font-family:inherit; font-size: 100%;}{\\text{EVPPI}}}",
          evsi: "{\\style{font-family:inherit; font-size: 100%;}{\\text{EVSI}}}"
        }
      }
    });
    </script>
    <link rel="stylesheet" href="../assets/beamer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">





class: title-slide

# Session 2.1: Bayesian inference&lt;span style="display:block; margin-top: 10px ;"&gt;&lt;/span&gt; 

## 

###     

### Bayesian modelling for Spatial and Spatio-temporal data, Imperial College 

&lt;!-- Can also separate the various components of the extra argument 'params', eg as in 
### Bayesian modelling for Spatial and Spatio-temporal data, Imperial College, , MSc in Epidemiology
--&gt;



---

layout: true  

.my-footer[ 
.alignleft[ 
&amp;nbsp; &amp;copy; Marta Blangiardo | Monica Pirani 
]
.aligncenter[
MSc in Epidemiology 
]
.alignright[
Bayesian modelling for Spatial and Spatio-temporal data, NA 
]
] 


&lt;style&gt;
pre {
  overflow-x: auto;
}
pre code {
  word-wrap: normal;
  white-space: pre;
}
&lt;/style&gt;


---

# Learning Objectives

After this session you should be able to:

&lt;span style="display:block; margin-top: 10px ;"&gt;&lt;/span&gt;

- Understand how Bayes Theorem can be applied to random variables

&lt;span style="display:block; margin-top: 10px ;"&gt;&lt;/span&gt;

- Describe what conjugacy means

&lt;span style="display:block; margin-top: 10px ;"&gt;&lt;/span&gt;

- Obtain posterior distribution for the Beta-Binomial and Gamma-Poisson families

&lt;span style="display:block; margin-top: 10px ;"&gt;&lt;/span&gt;

The topics treated in this lecture are presented in Chapter 3  of the book Blangiardo and Cameletti (2015) and in Chapter 2.3, 3.1-3.4 and 5.2-5.3 of the book Johnson, Ott, and Dogucu (2022).

---

# Outline 

1\. [Bayes Theorem for random variables](#BayRandom)

&lt;span style="display:block; margin-top: 30px ;"&gt;&lt;/span&gt;


2\. [A quick recap](#Recap)

&lt;span style="display:block; margin-top: 30px ;"&gt;&lt;/span&gt;

3\. [What is conjugacy?](#Conjugacy)

&lt;span style="display:block; margin-top: 30px ;"&gt;&lt;/span&gt;

4\. [Some conjugacy models: Beta-Binomial](#Beta-Binomial)

&lt;span style="display:block; margin-top: 30px ;"&gt;&lt;/span&gt;

5\. [Some conjugacy models: Gamma-Poisson](#PoissonGamma)


---

name: BayRandom

&lt;span style="display:block; margin-top: 250px ;"&gt;&lt;/span&gt;

.myblue[.center[.huge[
**Bayes Theorem for random variables**]]]


---

# An example

- A clinical trial is carried out to assess the efficacy of a preventive treatment for migraine
&lt;span style="display:block; margin-top: 20px ;"&gt;&lt;/span&gt;
- A group of 20 patients are offered the new drug and they have to report if they have a migraine episode in the next week after taking the medication.
&lt;span style="display:block; margin-top: 20px ;"&gt;&lt;/span&gt;
- Our aim is to estimate the probability of success of this new drug `\((\theta)\)`
&lt;span style="display:block; margin-top: 20px ;"&gt;&lt;/span&gt;
- Our data here is `\(Y\)`: the number of patients reporting no migraine episodes.
&lt;span style="display:block; margin-top: 20px ;"&gt;&lt;/span&gt;
Note that `\(Y\)` is a **random variable** (look at recording 3 for a more detailed description of this concept)

---

# Prior probability model

As a first step we need to assign a prior on `\(\theta\)`. 

As a simplification let's assume that `\(\theta\)` is discrete and can only get thevalues 0.1,0.4,0.8 with the following probability function, which specifies the prior probability of each possible `\(\theta\)` value:
&lt;span style="display:block; margin-top: 30px ;"&gt;&lt;/span&gt;

&lt;table class=" lightable-classic" style='font-family: "Arial Narrow", "Source Sans Pro", sans-serif; width: auto !important; margin-left: auto; margin-right: auto;'&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:right;"&gt; \(\theta\) &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; \(p(\theta)\) &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 0.1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.2 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 0.4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.5 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 0.8 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.3 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;span style="display:block; margin-top: 30px ;"&gt;&lt;/span&gt;

Note that this prior reflects some sort of information from a previous study on a similar compound and put 50% probability on the event that 40% of the patients will report a reduction in migraine.


---

# The Binomial data model

- Our random variable `\(Y\)` can take any discrete value between 0 and 20 (total number of patients)

- It will depend on the probability `\(\theta\)`

- For our formal Bayesian analysis, we must model this dependence of `\(Y\)` on `\(\theta\)` through a **conditional probability model**  

- We make two assumptions about the trials: (1) the outcome of any one patient doesnâ€™t influence the outcome of another; and (2) the probability of success does not change among patients

- We can use the Binomial model


.content-box-green[

`\(Y \sim \mbox{Binomial}(n, \theta)\)`

with conditional probability function
`$$p(y \mid \theta) = \frac{n!}{y!(n-y)!}\theta^{y}(1-\theta)^{n-y}$$`
- Mean of this distribution is `\(E(Y) = n \theta\)`
- Variance is `\(V(Y) = n \theta(1-\theta)\)`

.red[ Check out recording 4 for a recap on the Binomial distribution]
]

---

# The Binomial data model

This model allows to calculate ANY conditional probability. For instance, conditioning on `\(\theta=0.4\)` let's see the difference in the probability of getting 12 or 15 successes

1. `$$p(y=12 \mid \theta=0.4) = \frac{20!}{12!8!}0.4^{12}(1-0.4)^{8}= 0.035$$` 
2. `$$p(y=15 \mid \theta=0.4) = \frac{20!}{15!5!}0.4^{15}(1-0.4)^{5} = 0.0013$$`

-- 

Note you can get these results in R using


```r
&gt; #1
&gt; dbinom(12,20,0.4)
```

```
[1] 0.03549744
```

```r
&gt; #2
&gt; dbinom(15,20,0.4)
```

```
[1] 0.001294494
```
---

# Binomial likelihood function

- The Binomial provides a theoretical model of the data we might observe. 
- In the end we observe 10 successes out of the 20 patients `\((y=10)\)`.
- The next step in our Bayesian analysis is to determine how compatible this particular data is with the various possible `\(\theta\)`

.content-box-green[
We need to evaluate the *likelihood* of getting 10 successes in the trial under each possible value of `\(\theta\)`
]

Similarly to last week with events, the likelihood function follows from evaluating the conditional probability function `\(p(y=10\mid \theta)\)` for all the possible `\(\theta\)` values:

1. `$$p(y=10 \mid \theta=0.1) = \frac{20!}{10!10!}0.1^{10}(1-0.1)^{10}= 0.00000644$$` 
2. `$$p(y=10 \mid \theta=0.4) = \frac{20!}{10!10!}0.4^{10}(1-0.4)^{10} = 0.117$$`
3. `$$p(y=10 \mid \theta=0.8) = \frac{20!}{10!10!}0.8^{10}(1-0.8)^{10} = 0.002$$`

---

# Likelihood vs probability function

Let's recall the fundamental difference between probability function and likelihood function

&lt;span style="display:block; margin-top: 50px ;"&gt;&lt;/span&gt;

.pull-left[
.content-box-yellow[
When `\(\theta\)` is known, the **conditional probability function** `\(p(\cdot\mid \theta)\)` allows us to compare the probabilities of different values of the data `\(Y\)` occurring with `\(\theta\)`:
`$$p(y_1\mid \theta) \text{  vs  } p(y_2\mid \theta)$$`
]
]

.pull-right[
.content-box-yellow[
When `\(Y\)` is known, the **likelihood function** `\(L(\cdot\mid y) = p(y \mid \cdot)\)` allows us to compare the relative likelihood of the data `\(y\)` under different possible values of `\(\theta\)`:
`$$L(\theta_1 \mid y) = p(y \mid \theta_1) \text{  vs  } L(\theta_2 \mid y) = p(y \mid \theta_2)$$`
]
]

&lt;span style="display:block; margin-top: 50px ;"&gt;&lt;/span&gt;

So `\(L(\cdot \mid y)\)` provides the tool we need to evaluate the relative compatibility of data `\(Y=y\)` with various `\(\theta\)` values.

---

# Normalising constant

- Now we have a prior for `\(\theta\)` and a likelihood and as Bayesian we want to **balance** these two pieces of information to obtain the posterior.

- Something is missing...

--

The normalising constant!

This is the total probability of having `\(y=10\)` successes. How do we get this?

--

.content-box-green[
**Law of total probability**: 
`$$P(A) = \sum_j P(A \mid B_j)P(B_j) \text{   (for events)}$$`
`$$P(Y=y) = \sum_j p(Y \mid \theta_j)p(\theta_j) \text{   (for discrete probability functions)}$$`
]

&lt;span style="display:block; margin-top: 20px ;"&gt;&lt;/span&gt;


So we get `$$P(Y=10) = 0.00000644 \times 0.2 +  0.117 \times 0.5 + 0.002 \times 0.3 = 0.059$$`

Interpretation: across all the possible `\(\theta\)` there is only around 6% chance that there are 10 successes.
---

# Posterior distribition

Finally we are ready to apply Bayes theorem and get the posterior distribution

`$$p(\theta \mid y=10) = \frac{p(\theta)L(\theta \mid y=10)}{p(y=10)} \text{  for  } \theta \in \{0.1,0.4,0.8\}$$`
which gives us:

`$$p(\theta=0.1 \mid y=10) = \frac{0.2 \times 0.0000064}{0.059} = 0$$`
`$$p(\theta=0.4 \mid y=10) = \frac{0.5 \times 0.117}{0.059} = 0.99$$`

`$$p(\theta=0.8 \mid y=10) = \frac{0.3 \times 0.002}{0.059} = 0.01$$`
---

# Comparing prior and posterior
.pull-left[
&lt;table class=" lightable-classic" style='font-family: "Arial Narrow", "Source Sans Pro", sans-serif; width: auto !important; margin-left: auto; margin-right: auto;'&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:right;"&gt; \(\theta\) &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; \(p(\theta)\) &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; \(p(\theta \mid y=10)\) &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 0.1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.00 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 0.4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.5 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.99 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 0.8 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.3 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.01 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
]

.pull-right[

&lt;img src="./img/unnamed-chunk-4-1.png" style="display: block; margin: auto;" width="110%"&gt;
]
---

# Posterior shortcut

.pull-left[
- Moving forward we can actually forget about calculating the normalising constant

- Note that on slide 12 the `\(p(y=10)\)` appears at the denominator of all the `\(p(\theta \mid Y)\)`
&lt;span style="display:block; margin-top: 20px ;"&gt;&lt;/span&gt;
.box-content-green[
It normalises the posterior probabilities so they sum to 1
]

- So we can simply acknowledge that  `\(p(y=10) = 1/c\)` and replace the posterior as:

`$$p(\theta \mid y=10) \propto p(\theta)\times L(\theta \mid y)$$`
- The proportionality means that if we compare the normalised and unnormalised posterior they preserve their relative relationship
]

.pull-right[
&lt;img src="./img/unnamed-chunk-5-1.png" style="display: block; margin: auto;" width="110%"&gt;
]
---

name: recap

&lt;span style="display:block; margin-top: 250px ;"&gt;&lt;/span&gt;

.myblue[.center[.huge[
**A quick recap**]]]


---




# So a quick general recap: Bayesian inference

Makes fundamental distinction between 
- Observable quantities `\(y\)`, i.e.~the data
&lt;span style="display:block; margin-top: 20px ;"&gt;&lt;/span&gt;
- Unknown quantities `\(\theta\)`

&lt;span style="display:block; margin-top: 30px ;"&gt;&lt;/span&gt;

  - `\(\theta\)` can be statistical parameters, missing data, mismeasured data...
&lt;span style="display:block; margin-top: 20px ;"&gt;&lt;/span&gt;
`\(\rightarrow\)` parameters are treated as random variables
&lt;span style="display:block; margin-top: 20px ;"&gt;&lt;/span&gt;
`\(\rightarrow\)` in the Bayesian framework, we make probability statements about model parameters

--

&lt;span style="display:block; margin-top: 50px ;"&gt;&lt;/span&gt;

.red[Note that in the Frequentist framework, parameters are fixed non-random quantities and the probability statements concern the data]

---

# Bayesian inference

- As with any statistical analysis, we start building a model which specifies `\(p(Y=y \mid \theta)\)`
&lt;span style="display:block; margin-top: 20px ;"&gt;&lt;/span&gt;

- This is the .red[data distribution], which relates all variables into a .red[**full probability model**]
&lt;span style="display:block; margin-top: 20px ;"&gt;&lt;/span&gt;

- The choice of data distribution depends on the nature of the data:

e.g. are we analysing continuous or discrete data, are the data symmetric or skewed, etc.
&lt;span style="display:block; margin-top: 20px ;"&gt;&lt;/span&gt;

- As we observe the data, we can use descriptive tools (e.g. plots) to visualise the data and choose the best likelihood
&lt;span style="display:block; margin-top: 20px ;"&gt;&lt;/span&gt;

---

# Bayesian inference

From a Bayesian point of view
&lt;span style="display:block; margin-top: 20px ;"&gt;&lt;/span&gt;

-  `\(\theta\)` is unknown so should have a .red[probability distribution] reflecting our uncertainty about it before seeing the data
     
&lt;span style="display:block; margin-top: 20px ;"&gt;&lt;/span&gt;

`\(\rightarrow\)` need to specify a .redp[prior distribution] `\(p(\theta)\)`
&lt;span style="display:block; margin-top: 20px ;"&gt;&lt;/span&gt;

- `\(y\)` is known so we should condition on it
&lt;span style="display:block; margin-top: 20px ;"&gt;&lt;/span&gt;
`\(\rightarrow\)` use Bayes theorem to obtain conditional probability distribution for unobserved quantities of interest given the data:

`$$p(\theta \mid y)= \frac{ p(\theta)\, p(y \mid \theta)}
                         {\int  p(\theta)\,p(y \mid \theta)\,d\theta}
	\propto p(\theta)\,p(y \mid \theta)$$`
&lt;span style="display:block; margin-top: 20px ;"&gt;&lt;/span&gt;

This is the .red[posterior distribution]

--

- The prior distribution `\(p(\theta)\)`, expresses our uncertainty about `\(\theta\)` .red[before] seeing the data

- The posterior distribution `\(p(\theta \mid y)\)`, expresses our uncertainty about `\(\theta\)` .red[after] seeing the data


---

# The posterior distribution

Posterior distribution forms basis for all inference --- can
be summarised to provide
&lt;span style="display:block; margin-top: 10px ;"&gt;&lt;/span&gt;
- point and interval estimates of Quantities of Interest (QOI), e.g. treatment effect, small area estimates, ...
&lt;span style="display:block; margin-top: 10px ;"&gt;&lt;/span&gt;
- point and interval estimates of any function of the parameters
&lt;span style="display:block; margin-top: 10px ;"&gt;&lt;/span&gt;
- probability that QOI (e.g. treatment effect) exceeds a critical threshold
&lt;span style="display:block; margin-top: 10px ;"&gt;&lt;/span&gt;
- prediction of QOI in a new unit
&lt;span style="display:block; margin-top: 10px ;"&gt;&lt;/span&gt;
- prior information for future experiments, trials, surveys, ...
&lt;span style="display:block; margin-top: 10px ;"&gt;&lt;/span&gt;
- inputs for decision making
&lt;span style="display:block; margin-top: 10px ;"&gt;&lt;/span&gt;
- ...

---

name: Conjugacy

&lt;span style="display:block; margin-top: 250px ;"&gt;&lt;/span&gt;

.myblue[.center[.huge[
**What is conjugacy?**]]]


---

# How to select a prior

- Selecting the prior is crucial for a Bayesian analysis

.content-box-green[
- There is no right way to select a prior
- The choices often depend on the objective of the study and the nature of the data 
&lt;span style="display:block; margin-top: -10px ;"&gt;&lt;/span&gt;
]

- There are other criteria to consider when choosing a prior model:

**Computational ease**

Especially if we donâ€™t have access to computing power, it is helpful if the posterior model is easy to build.

**Interpretability**

The posterior balance (is a compromise) the data and the prior. A posterior model is interpretable, and thus more useful, when you can look at its formulation and identify the contribution of the data relative to that of the prior.

--

We introduce now a type of models which satisfy both properties

.content-box-yellow[
&lt;span style="display:block; margin-top: -20px ;"&gt;&lt;/span&gt;
### Conjugate prior
&lt;span style="display:block; margin-top: -20px ;"&gt;&lt;/span&gt;
Let the prior model for a parameter `\(\theta\)` with `\(p(\theta)\)` and the model of data `\(Y\)` conditioned on `\(\theta\)` have likelihood function `\(L(\theta \mid y)\)`.

If the resulting posterior model `\(p(\theta \mid y) \propto p(\theta) \times p(y \mid \theta)\)` is of the same family as the prior, then we say it is a **conjugate prior**.
&lt;span style="display:block; margin-top: -10px ;"&gt;&lt;/span&gt;
]


---

name: Beta-Binomial

&lt;span style="display:block; margin-top: 250px ;"&gt;&lt;/span&gt;

.myblue[.center[.huge[
**Some conjugacy models: Beta-Binomial**]]]

---


# Beta-Binomial model for proportions: example

- We consider an early investigation of a new drug
&lt;span style="display:block; margin-top: 30px ;"&gt;&lt;/span&gt;
- Experience with similar compounds has suggested that response rates between 0.2 and 0.6 could be feasible
&lt;span style="display:block; margin-top: 30px ;"&gt;&lt;/span&gt;
- We interpret this as a distribution with mean = 0.4 and standard deviation 0.1 
&lt;span style="display:block; margin-top: 30px ;"&gt;&lt;/span&gt;
- A Beta(9.2,13.8) distribution has these properties (Check recording 7 to see how to go from the .red[mean and sd] to the .red[a and b] parameters of a Beta distribution)
&lt;span style="display:block; margin-top: 30px ;"&gt;&lt;/span&gt;
- Suppose we now treat `\(n=20\)` volunteers with the compound and observe `\(y=15\)` positive responses

---

# Identifying the different model components

- Assuming patients are independent, with common unknown response rate `\(\theta\)`, leads to a binomial data distribution
`\begin{align}
p(y \mid  n, \theta) &amp;= \left( \begin{array}{c} n \\ y \end{array} \right) \theta^y (1-\theta)^{n-y} \; \propto \; \theta^y (1-\theta)^{n-y}
\end{align}`


- `\(\theta\)`  needs a continuous prior distribution:

`\begin{align}
\theta &amp; \sim  \hbox{Beta}(a,b )\\
p(\theta) &amp;= \frac{\Gamma (a+b)}{\Gamma (a) \Gamma(b)} \; \theta^{a-1} (1-\theta)^{b-1}
\end{align}`

---

# Combining prior and data

Combining the Binomial data and the Beta prior gives the following posterior distribution

.content-box-green[
`\begin{align}
p(\theta \mid y, n)  &amp; \propto  p(y \mid \theta, n) p(\theta)\\
&amp; \propto  \theta^y (1-\theta)^{n-y} \theta^{a-1} (1-\theta)^{b-1}\\
&amp; =  \theta^{y+a-1} (1-\theta)^{n-y+b-1}
\end{align}`
]

The posterior is still a Beta distribution (with different parameters):

`$$p(\theta \mid y, n)  \propto  \hbox{Beta}(y+a, \; n-y+b)$$`
---

# Comparing prior, likelihood and posterior

&lt;img src="./img/unnamed-chunk-6-1.png" style="display: block; margin: auto;" width="40%"&gt;

---

# Gamma-Poisson model for count data: example

- .red[For a recap on the Poisson distribution see recording 5]

&lt;span style="display:block; margin-top: 20px ;"&gt;&lt;/span&gt;

- In epidemiology we are often interested in estimating the .red[rate] or .red[relative risk] rather than the .red[mean] for Poisson data:

&lt;span style="display:block; margin-top: 20px ;"&gt;&lt;/span&gt;

  - Suppose we observe `\(y=7\)` cases of leukaemia in one region;
  - The expected number of cases is `\(E=4\)`

&lt;span style="display:block; margin-top: 20px ;"&gt;&lt;/span&gt;

  - **Data distribution**: Poisson with mean `\(\theta = \lambda \times E\)`, where `\(\lambda\)` is the unknown incidence ratio:
  
`$$p(y \mid  \lambda, E) = \frac{(\lambda E)^{y}e^{-\lambda E} }{y!}$$`
&lt;span style="display:block; margin-top: 20px ;"&gt;&lt;/span&gt;

  - **Prior**: `\(\mathrm{Gamma}(a, b)\)` on the the risk `\(\lambda\)`:
$$p(\lambda)  =  \frac{b^a}{\Gamma(a)} \lambda^{a-1} e^{-b \lambda} $$

.red[Check recording 8 for a recap on the Gamma distribution]
---

# Combining likelihood and prior

This implies the following posterior

`\begin{align*}
p(\lambda \mid y) &amp; \propto p(\lambda)\,p(y \mid \lambda) \\
  &amp; = \frac{b^a}{\Gamma(a)}\lambda^{a-1}e^{-b(\lambda)}e^{-(\lambda E)}\frac{(\lambda E)^{y}}{y!}\\
  &amp; \propto  \lambda^{a+y-1} e^{-(b+E)\lambda}\\
 &amp; =  \mathrm{Gamma}(a + y ,\, b + E)
\end{align*}`

--

The posterior is another (different) Gamma distribution

`$$E(\theta \mid y)= \frac{a + y }{ b + E}$$`

So posterior mean depends on the prior `\((a, b)\)` and on the data `\((y, E)\)`


---

# Prior, likelihood, posterior for the Leukaemia example

.pull-left[

- Assuming a prior `\(\lambda \sim \mathrm{Gamma}(2,2)\)`
&lt;span style="display:block; margin-top: 20px ;"&gt;&lt;/span&gt;

- Considering the data `\(y=7, E=4\)` 
&lt;span style="display:block; margin-top: 20px ;"&gt;&lt;/span&gt;

- We obtain a posterior `\(\lambda \mid y \sim \mathrm{Gamma}(9,6)\)` centered around 1.5

]

.pull-right[

&lt;center&gt;&lt;img src=./img/PoissonGamma.png width='75%' title='INCLUDE TEXT HERE'&gt;&lt;/center&gt;

The posterior becomes a compromise between the prior and the data
]

---

# References

Blangiardo, M. and M. Cameletti (2015). _Spatial and spatio-temporal Bayesian models with R-INLA_. John Wiley &amp; Sons.

Johnson, A. A., M. Q. Ott, and M. Dogucu (2022). _Bayes Rules!: An Introduction to Applied Bayesian Modeling_. CRC Press.
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="https://platform.twitter.com/widgets.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
