---
title: "Session 1.2: Bayes Theorem"
params: 
   conference: "Bayesian modelling for Spatial and Spatio-temporal data"
   location: "Imperial College"
   date: ""
   short_title: "MSc in Epidemiology"
output:
  xaringan::moon_reader: 
    includes: 
       # This line adds a logo based on the format selected in the file 'assets/include_logo.html'
       in_header: "../assets/latex_macros.html" 
       # NB: the actual options (eg placement of the logo and actual logo file) can be changed there
       #after_body: "../assets/insert-logo.html"
    seal: false
    yolo: no
    lib_dir: libs
    nature:
      beforeInit: ["https://platform.twitter.com/widgets.js"]
      navigation:
        scroll: false # disable slide transitions by scrolling
      highlightStyle: github
      highlightLines: yes
      countIncrementalSlides: no
      ratio: '16:9'
      titleSlideClass:
      - center
      - middle
    self_contained: false 
    css:
    - "../assets/beamer.css"
editor_options: 
  chunk_output_type: console
---

```{r echo=F,message=FALSE,warning=FALSE,comment=NA}
# Sources the R file with all the relevant setup and commands
source("../assets/setup.R")

# Stuff from 'xaringanExtra' (https://pkg.garrickadenbuie.com/xaringanExtra)
# This allows the use of panels (from 'xaringanExtra')
xaringanExtra::use_panelset()
# This allows to copy code from the slides directly
#xaringanExtra::use_clipboard()
# This freezes the frame for when there's a gif included
#xaringanExtra::use_freezeframe()

# Defines the path to the file with the .bib entries (in case there are references)
#bibfile=RefManageR::ReadBib("~/Dropbox/Books/INLABook/ShortCourse/VIBASS/Biblio.bib",check = FALSE)
bibfile=ReadBib("~/Dropbox/Bayes_Spatial_2023/Material/Biblio.bib",check = FALSE)
#bibfile=ReadBib("~/Dropbox/Lavori condivisi/2015_Book/ShortCourse/VIBASS/Biblio.bib",check = FALSE)
```

class: title-slide

# `r rmarkdown::metadata$title``r vspace("10px")` `r rmarkdown::metadata$subtitle`

## `r rmarkdown::metadata$author`

### `r rmarkdown::metadata$institute`    

### `r rmarkdown::metadata$params$conference`, `r rmarkdown::metadata$params$location` 

<!-- Can also separate the various components of the extra argument 'params', eg as in 
### `r paste(rmarkdown::metadata$params, collapse=", ")`
-->

`r ifelse(is.null(rmarkdown::metadata$params$date),format(Sys.Date(),"%e %B %Y"),rmarkdown::metadata$params$date)`

---

layout: true  

.my-footer[ 
.alignleft[ 
&nbsp; &copy; Marta Blangiardo | Monica Pirani 
]
.aligncenter[
`r rmarkdown::metadata$params$short_title` 
]
.alignright[
`r rmarkdown::metadata$params$conference`, `r short_date` 
]
] 

```{css,echo=FALSE, eval=FALSE}
.red {
  color: red;
}
.blue {
  color: 0.14 0.34 0.55;
}

.content-box-blue { background-color: #F0F8FF; }

}

.scrollable {
  height: 80%;
  overflow-y: auto;
}

```
<style>
pre {
  overflow-x: auto;
}
pre code {
  word-wrap: normal;
  white-space: pre;
}
</style>


---

# Learning objectives

After this lecture you should be able to 
`r vspace("40px")`
- Distinguish between conditional probability and likelihood      
`r vspace("40px")`
- Compute joint and conditional probabilities
`r vspace("40px")`
- Use Bayes theorem to obtain posterior probabilities

`r vspace("20px")`

The topics treated in this lecture are presented in Chapter 3  of `r Citet(bibfile, "blangiardo2015spatial")` and in Chapter 2 of  `r Citet(bibfile, "johnson2022bayes")`.

---

# Outline 

`r vspace("30px")`

1\. [Conditional probability and likelihood](#Cond_lik)

`r vspace("30px")`

2\. [Normalising constant](#Norm)

`r vspace("30px")`

3\. [Bayes Theorem](#BayTheo)

---

name: Cond_lik
  
`r vspace("250px")`

.myblue[.center[.huge[
**Conditional probability and likelihood**]]]

---

# Example: COVID-19 test

- A COVID-19 test has shown to have 80% sensitivity and 99% specificity 

- In England, COVID-19 prevalence is 6%
`r vspace("50px")`

<center>
.content-box-green[What is the chance that a patient testing positive actually does have COVID-19?]
</center>

`r vspace("100px")`

```{r eval=TRUE, echo=FALSE, include=FALSE}
COVID_test <- data.frame(Pos_Test=c(0.8,0.01),Neg_test=c(0.2,0.99), row.names=c("Disease","No Disease"))
COVID_test
COVID_prev <- 0.06
```
--
We have two pieces of information:

1. Our prior suggests that the COVID-19 prevalence in the country is low (6%)
2. Our data suggest that our diagnostic test is accurate
---

# Example: COVID-19 test

How can we balance these two pieces of information to answer the question about having the disease?

`r vspace("50px")`

`r include_fig("Doodle.png",width="75%")`

---

# Prior probability model

- Let's look at our prior: COVID-19 prevalence in the country is 6%. How can we formalise it?
`r vspace("50px")`

Define A as the event: **a person has COVID-19 in England**

Then $P(A)=0.06$ and consequently $P(A^C)=0.94$.
`r vspace("50px")`

Remember that a valid probability model must: 
1. account for all possible events (having or not having COVID-19); 
2. assign prior probabilities to each event.

`r vspace("20px")`

Also 
3. each probability must be between 0 and 1;
4. these probabilities must sum to one.

---


# Conditional probability 

Now we summarise the **data** that we get from the diagnostic tests:

- 80% sensitivity: if a person has COVID-19 they will test positive 80 out of 100 times

- 99% specificity: if a person does not have COVID-19 they will test negative 99 out of 100 times 

--

These are .alert[**conditional probabilities**] and defining B the event: a person tests positive for COVID-19, we can summarise the above information in  

$$P(B \mid A) = 0.8 \text{    and    } P(B \mid A^C)=0.01$$

---

# Some rules of conditional probabilities

In general, comparing the conditional vs unconditional probabilities $P(B\mid A)$ vs $P(B)$, reveals the extent to which information about $B$ changes in light of $A$ 

In some cases, the certainty of an event $B$ might increase in light of new data $A$: 

- if you eat hamburgers and chips every day, your probability of having high colesterol is higher than in the general population

$$P(B \mid A) > P(B)$$

--

In some cases, the certainty of an event $B$ might decrease in light of new data $A$: 

- if you are vaccinated against flu, your probability of getting into hospital with serious flu complications decreases

$$P(B \mid A) < P(B)$$
--

The order of conditioning is also important, as generally $P(B \mid A) \neq P(A \mid B)$: for instance in India the probability of getting bitten by a snake after a week of torrential rain is $P(B \mid A)=0.4$; but this does not mean that there is a 0.4 probability of a week of torrential rain after someone is bitten by a snake $P(A \mid B)$. 

--

Finally information about A does not always change our understanding of B: then the two events are **independent** and $P(B \mid A) = P(B)$
`r vspace("20px")`

--

.red[See recording 2 for an additional recap on Probability]

---

# Some rules of conditional probabilities

- Provable from probability axioms

$$ P(A|B) =\frac{P(A \cap B)}{P(B)} = \frac{ P(B|A) P(A) } {P(B)}$$

- If $A_i$ is a set of mutually exclusive and exhaustive events
(*i.e.* $A_i\cap A_j=\emptyset$, $P( \bigcup\limits_i A_i ) = \sum\limits_i P(A_i) = 1$), then

$$ P(A_i|B) = \frac{ P(B|A_i) P(A_i) } {P(B)} = \frac{ P(B|A_i) P(A_i) } {\sum\limits_j P(B|A_j) P(A_j) }$$

---

# Likelihood

Let's re-examine the COVID-19 example: we know that if someone has the disease their probability of testing positive is much higher than if they do not (0.8 vs 0.01), so what we think is that the probability of having the disease if testing positive must be high. 

We are moving unconsciously from conditional probability to likelihood.

--

When A is known, the conditional probability function $P(.\mid A)$ allows to compute the probabilities of an unknown event $B$ or $B^C$:
$$P(B\mid A) \text{ vs } P(B^C \mid A)$$
When B is known (i.e. observed), the likelihood function $L(.\mid B)$ allows to evaluate the relative compatibility of the data $B$ with the event $A$ or $A^C$:

$$L(A \mid B) = P(B \mid A) \text{   vs   } L(A^C \mid B) = P(B \mid A^C)$$

--

So far we have (i) the prior evidence of getting COVID-19 and (ii) the likelihood which tells that a positive test is more likely among diseased people:

```{r table, echo=FALSE, eval=TRUE}
library(kableExtra)
df<- data.frame(Event=c("Prior","Likelihood"),A=c(0.06, 0.8),Ac=c(0.94,0.01),Total=c(1,0.81))
kable(df, col.names = c("Event", "A", "A^C", "Total"), "pipe", escape = F, caption = "Prior probability and likelihood",  booktabs=TRUE) %>% kable_styling(latex_options = "hold_position")
```
---

name: Norm
  
`r vspace("250px")`

.myblue[.center[.huge[
**Normalising constant**]]]

---

# Normalising constant

The marginal probability of testing positive $P(B)$ provides an important point of comparison. This is the last bit of information we need. Let's try to fill in the table below:

`r include_fig("Table_Prob.png",width="50%")`

First let's look at the row A: there are those who tests positive AND have the disease and those who do not test positive AND have the disease. To get these probabilities remember that $P(A)=0.06$ and that $P(B\mid A)=0.8$, so that 

1. $P(A \cap B) = 0.06 \times 0.8 = 0.048$
Then using a similar rationale we can get:
2. $P(A \cap B^C) = P(B^C\mid A) \times P(A) = (1-0.8)\times 0.06 = 0.012$
3. $P(A^C\cap B) = P(B\mid A^C) \times P(A^C) = 0.01\times 0.94 = 0.009$
4. $P(A^C \cap B^C) = P(B^C\mid A^C) \times P(A^C) = 0.99 \times 0.94 = 1- 0.048 - 0.012 - 0.0094 = 0.931$

And finally the $P(B) = 0.048 + 0.0094 = 0.057$
---

count: false

# Normalising constant

The marginal probability of testing positive $P(B)$ provides an important point of comparison. This is the last bit of information we need. Let's try to fill in the table below:

`r include_fig("Table_Bay2.png",width="50%")`

First let's look at the row A: there are those who tests positive AND have the disease and those who do not test positive AND have the disease. To get these probabilities remember that $P(A)=0.06$ and that $P(B\mid A)=0.8$, so that 

1. $P(A \cap B) = 0.06 \times 0.8 = 0.048$
Then using a similar rationale we can get:
2. $P(A \cap B^C) = P(B^C\mid A) \times P(A) = (1-0.8)\times 0.06 = 0.012$
3. $P(A^C\cap B) = P(B\mid A^C) \times P(A^C) = 0.01\times 0.94 = 0.009$
4. $P(A^C \cap B^C) = P(B^C\mid A^C) \times P(A^C) = 0.99 \times 0.94 = 1- 0.048 - 0.012 - 0.0094 = 0.931$

And finally the $P(B) = 0.048 + 0.0094 = 0.057$
---

name: BayTheo
  
`r vspace("250px")`

.myblue[.center[.huge[
**Bayes Theorem**]]]

---

# Now we put all together...

We are now ready to answer the question: 
`r vspace("10px")`

<center>
.content-box-green[What is the chance that a patient testing positive actually does have COVID-19?]
</center>

`r vspace("10px")`

Going back to the table we can zoom in into the people testing positive  

`r include_fig("Table_Bay3.png",width="40%")`

and using the conditional probability rules we get 
$$P(A \mid B) = \frac{P(A \cap B)}{P(B)} = 0.048 / 0.057 = 0.84$$


<center>.red[**This is building Bayes Theorem from scratch.**]</center>


Now remember that 
$$P(A \cap B) = P(B \mid A) P(A)$$

then Bayes' theorem will calculate $P(A \mid B)$ combining information from the prior $P(A)$ and the likelihood of observing $A$ with the event $B$, given by $P(B \mid A)$

---

# Does it really work?
`r vspace("10px")`

```{r,engine='tikz', echo=F, out.width="50%",opts=list(width="50%",title="INSERT TEXT HERE"),eval=TRUE}
\usetikzlibrary{shapes,arrows,decorations.pathreplacing,positioning,calc}
\begin{tikzpicture}
\draw(2,4.1) node[align=center,rectangle,rounded corners=2ex,draw=none,fill=none,font=\sffamily\fontsize{7}{7}\selectfont,minimum width=2.2cm](1){\color{red} Disease};
\draw(4.5,4.1) node[align=center,rectangle,rounded corners=2ex,draw=none,fill=none,font=\sffamily\fontsize{7}{7}\selectfont,minimum width=2.2cm](2){\color{green!70!black!90}No disease};

\draw(2,3.8) node[align=center,rectangle,rounded corners=2ex,draw=none,fill=none,font=\sffamily\fontsize{7}{7}\selectfont,minimum width=1.2cm](3){$p(A)=0.06$};

\draw(4.5,3.8) node[align=center,rectangle,rounded corners=2ex,draw=none,fill=none,font=\sffamily\fontsize{7}{7}\selectfont,minimum width=1.2cm](4){$P(A^C)=0.94$};

\draw(2.0,2) node[align=center,rectangle,rounded corners=2ex,draw=none,fill=none,font=\sffamily\fontsize{7}{7}\selectfont,minimum width=1.2cm](5){\textbf{$P(B^C)$}};

\draw(2.0,1.7) node[align=center,rectangle,rounded corners=2ex,draw=none,fill=none,font=\sffamily\fontsize{7}{7}\selectfont,minimum width=1.2cm](7){Negative Test};

\draw(4.5,2) node[align=center,rectangle,rounded corners=2ex,draw=none,fill=none,font=\sffamily\fontsize{7}{7}\selectfont,minimum width=1.2cm](6){$P(B)$};

\draw(4.5,1.75) node[align=center,rectangle,rounded corners=2ex,draw=none,fill=none,font=\sffamily\fontsize{7}{7}\selectfont,minimum width=1.2cm](8){Positive Test};

\draw [->,>=latex,shorten >=0pt,auto,node distance=0cm,ultra thin,color=red] (3.south) -- (5.north) node[midway,left,font=\fontsize{6}{7}\selectfont] {$0.2$}; 
\draw [->,>=latex,shorten >=0pt,auto,node distance=0cm,ultra thin,color=red] (3.300) -- (6.120) node[right,below,pos=.72,inner sep=5pt,font=\fontsize{6}{7}\selectfont] {$0.8$}; 
\draw [->,>=latex,shorten >=0pt,auto,node distance=0cm,ultra thin,color=green!70!black!90] (4.south) -- (6.north) node[midway,right,font=\fontsize{6}{7}\selectfont] {$0.01$}; 
\draw [->,>=latex,shorten >=0pt,auto,node distance=0cm,ultra thin,color=green!70!black!90] (4.240) -- (5.60) node[right,above,pos=.74,inner sep=5pt,font=\fontsize{6}{7}\selectfont] {$0.99$}; 
\end{tikzpicture}
```
--

Usin Bayes' theorem we get

$$p(A|B) = \frac{ P(B|A) P(A) } {P(B|A) P(A) + P(B|A^C) P(A^C)}=\frac{0.8 \times 0.06 } {0.8 \times  0.06 + 0.01 \times 0.94} = 0.84$$

---

# Comments

.pull-left[

- The disease prevalence can be thought of as a *prior* probability ( $p$ = 0.06)

`r vspace("20px")`

- Observing a positive result causes us to modify this probability to $p$ = 0.84. This is our *posterior* probability that patient is COVID-19 positive.

]

.pull-right[

`r include_fig("Bayes.png",width="80%",title="")`
]

--

- Bayes theorem applied to *observables* (as in diagnostic testing) is uncontroversial and established
- More controversial in general statistical analyses: *parameters* are unknown quantities, and prior distributions need to be specified $\rightarrow$ .red[Bayesian inference]

- Stay tuned, we are going to dive into that next week!
---

# References

```{r refs, echo=FALSE, results="asis"}
PrintBibliography(bibfile,.opts=list(max.names=3))
```
