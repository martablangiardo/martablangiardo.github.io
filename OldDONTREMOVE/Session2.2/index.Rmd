---
title: "Session 2.2: Posterior Predictive Distribution and Monte Carlo computation"
params: 
   conference: "Bayesian modelling for Spatial and Spatio-temporal data"
   location: "Imperial College"
   date: ""
   short_title: "MSc in Epidemiology"
output:
   xaringan::moon_reader: 
    includes: 
       # This line adds a logo based on the format selected in the file 'assets/include_logo.html'
       in_header: "assets/latex_macros.html" 
       # NB: the actual options (eg placement of the logo and actual logo file) can be changed there
     #  after_body: "assets/insert-logo.html" # Monica commented this
    seal: false
    yolo: no
    lib_dir: libs
    nature:
      beforeInit: ["assets/remark-zoom.js","https://platform.twitter.com/widgets.js"]
      navigation:
        scroll: false # disable slide transitions by scrolling
      highlightStyle: github
      highlightLines: yes
      countIncrementalSlides: no
      ratio: '16:9'
      titleSlideClass:
      - center
      - middle
    self_contained: false 
    css:
    - "assets/beamer.css"
editor_options: 
  chunk_output_type: console
---

```{r global_options, echo = FALSE, include = FALSE}
options(width = 999)
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE,
                      cache = FALSE, tidy = FALSE, size = "small")
#options(htmltools.preserve.raw = FALSE)
# https://stackoverflow.com/questions/65766516/xaringan-presentation-not-displaying-html-widgets-even-when-knitting-provided-t
```

```{r echo=F, message=FALSE, warning=FALSE, comment=NA}
source("assets/setup.R")
library(INLA)
xaringanExtra::use_panelset()
bibfile=ReadBib("~/Dropbox/Bayes_Spatial_2023/Material/Biblio.bib",check = FALSE)
#bibfile=RefManageR::ReadBib("~/Dropbox/TEACHING/YEAR_2023/Bayes_Spatial_2023/Material/Session2.2/Biblio.bib",check = FALSE)
```

class: title-slide

# `r rmarkdown::metadata$title``r vspace("10px")` `r rmarkdown::metadata$subtitle`

## `r rmarkdown::metadata$author`

### `r rmarkdown::metadata$institute`    

### `r rmarkdown::metadata$params$conference`, `r rmarkdown::metadata$params$location` 

<!-- Can also separate the various components of the extra argument 'params', eg as in 
### `r paste(rmarkdown::metadata$params, collapse=", ")`
-->

`r ifelse(is.null(rmarkdown::metadata$params$date),format(Sys.Date(),"%e %B %Y"),rmarkdown::metadata$params$date)`

---

layout: true  

.my-footer[ 
.alignleft[ 
&nbsp; &copy; Marta Blangiardo | Monica Pirani
]
.aligncenter[
`r rmarkdown::metadata$params$short_title` 
]
.alignright[
`r rmarkdown::metadata$params$conference`, `r short_date` 
]
] 

```{css,echo=FALSE, eval=FALSE}
.red {
  color: red;
}
.blue {
  color: 0.14 0.34 0.55;
}

.content-box-blue { background-color: #F0F8FF; }

}

.scrollable {
  height: 80%;
  overflow-y: auto;
}

```
<style>
pre {
  overflow-x: auto;
}
pre code {
  word-wrap: normal;
  white-space: pre;
}
</style>


---

# Learning objectives

After this lecture you should be able to 
`r vspace("40px")`
- Describe what the posterior predictive distribution (PPD) is  
`r vspace("40px")`
- Explain how the PPD is computable
`r vspace("40px")`
- Describe what Monte Carlo simulation is, and why it is useful

`r vspace("30px")`


- The topic of posterior prediction is treated in Section 8.3 of `r Citet(bibfile, "johnson2022bayes")`

- The topic of Monte Carlo simulation is presented in Sections 4.1-4.4  of `r Citet(bibfile, "blangiardo2015spatial")`.


---

# Outline 

`r vspace("30px")`

1\. [Bayesian predictive distribution](#Bayesian predictive distribution)

`r vspace("30px")`

2\. [Computation of PPD](#Computation of PPD)


`r vspace("30px")`

3\. [Introduction to Bayesian computing using Monte Carlo simulation](#Introduction to Bayesian computing using Monte Carlo simulation)


`r vspace("30px")`

4\. [Example of MC computation](#Example of MC computation)

---

name: Bayesian prediction
  
`r vspace("250px")`

.myblue[.center[.huge[
**Posterior Predictive Distribution**]]]

---

# Bayesian prediction


- Often the objective of our analysis is to predict a future event, based upon the data currently available.

`r vspace("20px")`
--

- Consider this example: 

`r vspace("20px")`

We estimate the prevalence of a disease in a UK hospital using a sample of n = 58 individuals. 
`r vspace("10px")`
We find that $y = 10$ individuals have the disease. 
`r vspace("10px")`
.red[What is the probability that, if we additionally sample $k = 30$ individuals this year, at least 5 will have the disease?]

--

`r vspace("20px")`

- As usual we start specifying the data distribution:

$$y \sim \mathrm{Binomial}(\theta,n=58)$$


- Let's  $\theta$ be the true disease prevalence and $y^{*}$ be the .red[predicted value] 

--

- If $\theta$ were known, then we would predict $$y^*|\theta\sim \mbox{Binomial}(30,\theta)$$
	 thus $\mbox{P}(y\geq{5}) = 1-(\sum_{j=0}^4 \theta^{j}(1-\theta)^{30-j})$
	 
.blue[BUT ...] $\theta$ is unknown

---


# Source of variation in prediction:

- We don't know the true value of the parameters and we specify a prior on it:

$$\theta \sim \mathrm{Beta}(a,b)$$

- There is sampling variability ( $\rightarrow$ choice of the data distribution)


--


To account for the sources of variation we iterate the following steps:


1. Sample from the posterior distribution $\theta \sim p(\theta\mid y)$

2. Sample new values $y^*\sim p(y\mid \theta)$

- By repeating these steps a large number of times, we eventually obtain a reasonable approximation to the .red[posterior predictive distribution].

---

# Posterior Predictive Distribution (PPD)

- The .red[PPD] represents our uncertainty over the outcome of a future data collection, accounting for the observed data and model choice


- For the sake of prediction, the parameters are not of interest. They are vehicles by which the data inform about the predictive model


- The .red[PPD] averages over their posterior uncertainty

	$$p(y^*|y) = \int p(y^*|\theta)p(\theta|y)d\theta$$
- This properly accounts for parametric uncertainty


- The input is data, the output is a prediction distribution


---

name: Computation of PPD
  
`r vspace("250px")`

.myblue[.center[.huge[
**Computation**]]]


---
# Computing the PPD


- Say $\theta^{(1)},...,\theta^{(M)}$ are samples from the posterior


- If we make a sample for $y^*$ for each $\theta^{(m)}$, $$y^{*(m)}\sim p(y|\theta^{(m)})$$ then the $y^{*(m)}$ are samples from the PPD


- The posterior predictive mean is approximated by the sample mean of the $y^{*(m)}$


- The probability that $y^{*}\geq 5$ is approximated by the sample proportion of the $y^{*(m)}$ that are equal or above 5


---

name: Example
  
`r vspace("250px")`

.myblue[.center[.huge[
**Example**]]]

---

# Example

- We estimate the prevalence of a disease in the UK population using a sample of $n=58$ individuals.

- We find that $y=10$ individuals have the diseases. 

- .red[What is the probability that, if we additionally sample $k=30$ individuals this year, at least 5 will have the disease?]
.pull-left[
1. .blue[Likelihood]: $y \sim \mbox{Binomial}(\theta,58)$
2. .blue[Prior]: $\theta \sim \mbox{Beta}(1,1)$ 
3. .blue[Posterior]: $\theta \mid y \sim \mbox{Beta}(10+1,58-10+1)$
4. .blue[PPD]: $y^* \sim \mbox{Binomial}(\theta \mid y,30)$
5. $P(y\geq{5}) = \sum_{j=5}^{30} P(y^*=j)$
]

.pull-right[
`r include_fig("PPD_example.png",width="60%",title="")`
]

`r vspace("-100px")`
```{r eval=TRUE, echo=FALSE,  fig.height = 3.8, fig.width = 4, fig.show='hold',opts=list(width="40%")}
set.seed(1234)
theta = rbeta(1000,11,49)
y = c()
for(i in 1:1000){
  y[i] = rbinom(1,30,theta[i])
}
p = sum(y>=5)

hist(y, col="yellow", main="")
abline(v=5, lty=2,lwd=1)
```


---

name: Introduction to Bayesian computing: Monte Carlo simulation
  
`r vspace("250px")`

.myblue[.center[.huge[
**Introduction to Bayesian computing: Monte Carlo simulations**]]]

---
# Bayesian computing

- In Session 2.1 we have introduced the concept of .red[conjugacy], and we said that if the the prior and posterior come from the same family of distributions, the prior is said to be **conjugate** to the likelihood $\rightarrow$ the posterior is a known distribution. 

`r vspace("10px")`

- In real life it is (almost) impossible to use conjugacy so we need to resort to simulative approaches or approximations to perform computation:

  - Monte Carlo methods
  `r vspace("10px")`

  - Markov Chain Monte Carlo (MCMC) methods
  `r vspace("10px")`

  - Integrated Nested Laplace Approximation (INLA)


---
# Monte Carlo simulation

- A Monte Carlo (MC) simulation is a randomly evolving simulation.


- MC sampling is based on the idea that if you have a large random sample from a certain distribution, the statistics that you can calculate in this sample (mean, standard deviation, percentiles...) will be very similar to the corresponding theoretical values in the distribution.


- If you have a complicated mathematical expression for a distribution and you cannot calculate algebraically important parameters, you could get the computer to generate a large random sample from such a distribution.


- By calculating the mean of that parameter in the sample you could estimate the mean in the original distribution with great precision.

---
# Example: a Monte Carlo approach to estimating tail-areas of distributions

Suppose we want to know the probability of getting 8 or more heads when we toss a fair coin 10 times.


- An *algebraic* approach would be:

`r include_fig("Coin_Alg.png",width="80%")`


- A *physical* approach would be to repeatedly throw a set of 10 coins
and count the proportion of throws that there were 8 or more heads.

---

- A *simulation* approach uses a computer to toss the coins!

`r include_fig("coin-toss.jpg",width="60%")`

`r vspace("10px")`

Proportion with 8 or more heads in 10 tosses: (a)  After 100 throws (0.02);  (b) after 10,000 throws (0.0577); (c) the true Binomial distribution (0.0547).

---

# Monte Carlo approach to approximate log-odds

- We start with a Binomial likelihood 
$$y \mid \theta \sim \text{Binomial}(\theta, n)$$
combined with a 
$$\text{Beta}(a,b)$$
as prior for the probability of success $\theta$.

`r vspace("10px")`

- We are interested in the log-odds function of $\theta$ defined as 
$$\log \left(\frac{\theta}{1-\theta}\right)$$

- The integral 
$$\int_0^1 \log\left(\frac{\theta}{1-\theta}\right) p(\theta\mid y)\text{d}\theta$$ 
cannot be computed analytically; we resort to Monte Carlo approximation.

---

# Example of MC in practice

- We simulate $m$ independent values $\left\{\theta^{(1)},\ldots, \theta^{(m)}\right\}$ from the 
$$\text{Beta}(a_1=y+a,b_1=n-y+b)$$ posterior distribution using the property of conjugacy (Beta prior is conjugate to the Binomial likelihood). 


- We apply the log-odds transformation to each value obtaining the set of values
$$\left\{\log\left(\frac{\theta^{(1)}}{1-\theta^{(1)}}\right),\ldots,\log\left(\frac{\theta^{(m)}}{1-\theta^{(m)}}\right)\right\}$$
- Finally, we compute the sample mean 
$$\frac{\sum_{i=1}^m \log\left(\frac{\theta^{(i)}}{1-\theta^{(i)}}\right)}{m}$$
which is the Monte Carlo approximation to 
$$\log \left(\frac{\theta}{1-\theta}\right)$$


---

# Example of MC: R code

In `R`:

```{r eval=TRUE, echo=TRUE}
a <- 1
b <- 1
theta <- rbeta(1,a,b)
n <- 1000
y <- rbinom(1, size=n, p=theta)
```

--

- With this setting the exact posterior distribution of $\theta$ is 

$$\text{Beta}(a_1=a+y,b_1=n-y+b)$$

- To approximate the log-odds, we simulate $m=50000$ values from this Beta posterior distribution using the `rbeta` function.

```{r eval=TRUE, echo=TRUE}
a1 <- a + y
b1 <- n - y + b
sim <- rbeta(n=50000, shape1=a1, shape2=b1) 
logodds <- log(sim/(1-sim))
```

---

# Results and comparison with the theoretical distribution

The empirical distribution of the Monte Carlo sample is plotted below together with the exact posterior distribution of $\theta$.

.pull-left[
`r include_fig("MCexample.png",width="80%",title="")`
]

.pull-right[
`r include_fig("MCexampleLogOdds.png",width="80%",title="")`
]







