---
title: "Session 10: spatio-temporal model for geostatistical data"
params: 
   conference: "Bayesian modelling for Spatial and Spatio-temporal data"
   location: "Imperial College London"
   date: ""
   short_title: "MSc Epidemiology"
output:
   xaringan::moon_reader: 
    includes: 
       # This line adds a logo based on the format selected in the file 'assets/include_logo.html'
       in_header: "assets/latex_macros.html" 
       # NB: the actual options (eg placement of the logo and actual logo file) can be changed there
     #  after_body: "assets/insert-logo.html" # Monica commented this
    seal: false
    yolo: no
    lib_dir: libs
    nature:
      beforeInit: ["assets/remark-zoom.js","https://platform.twitter.com/widgets.js"]
      navigation:
        scroll: false # disable slide transitions by scrolling
      highlightStyle: github
      highlightLines: yes
      countIncrementalSlides: no
      ratio: '16:9'
      titleSlideClass:
      - center
      - middle
    self_contained: false 
    css:
    - "assets/beamer.css"
editor_options: 
  chunk_output_type: console
---

```{r global_options, echo = FALSE, include = FALSE}
options(width = 999)
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE,
                      cache = FALSE, tidy = FALSE, size = "small")
#options(htmltools.preserve.raw = FALSE)
# https://stackoverflow.com/questions/65766516/xaringan-presentation-not-displaying-html-widgets-even-when-knitting-provided-t
```

```{r echo=F, message=FALSE, warning=FALSE, comment=NA}
source("assets/setup.R")
library(INLA)
xaringanExtra::use_panelset()
bibfile=RefManageR::ReadBib("~/Dropbox/TEACHING/YEAR_2023/Bayes_Spatial_2023/Material/Session10/Biblio.bib",check = FALSE)
```

class: title-slide

# `r rmarkdown::metadata$title``r vspace("10px")` `r rmarkdown::metadata$subtitle`

## `r rmarkdown::metadata$author`

### `r rmarkdown::metadata$institute`    

### `r rmarkdown::metadata$params$conference`, `r rmarkdown::metadata$params$location` 

<!-- Can also separate the various components of the extra argument 'params', eg as in 
### `r paste(rmarkdown::metadata$params, collapse=", ")`
-->

`r ifelse(is.null(rmarkdown::metadata$params$date),format(Sys.Date(),"%e %B %Y"),rmarkdown::metadata$params$date)`



---

layout: true  

.my-footer[ 
.alignleft[ 
&nbsp; &copy; Marta Blangiardo | Monica Pirani
]
.aligncenter[
`r rmarkdown::metadata$params$short_title` 
]
.alignright[
`r rmarkdown::metadata$params$conference`, `r short_date` 
]
] 

```{css,echo=FALSE, eval=FALSE}
.red {
  color: red;
}
.blue {
  color: 0.14 0.34 0.55;
}

.content-box-blue { background-color: #F0F8FF; }

}
```

```{css, echo=FALSE}
.scrollable {
  height: 80%;
  overflow-y: auto;
} 
```

---

# Learning Objectives

At the end of this session you should be able to:

`r vspace("20px")`

- know the definition of spatio-temporal process in the geostatistics framework;


`r vspace("20px")`


- use `R-INLA` for implementing a (separable) space-time geostatistical model.


`r vspace("30px")`


The topics covered in this lecture can be found in:

- Chapter 10 of the book **Geospatial Health Data: Modeling and Visualization with R-INLA and Shiny**  https://www.paulamoraga.com/book-geospatial/index.html

- Section 7.2 of the book **Spatio-Temporal Bayesian Models with `R-INLA`** https://sites.google.com/a/r-inla.org/stbook/

- Section 7.1 of the book **Advanced Spatial Modeling with Stochastic Partial Differential Equations Using R and INLA** https://becarioprecario.bitbucket.io/spde-gitbook/index.html

---

# Outline 

`r vspace("30px")`

1\. [Spatio-temporal processes + a space-time hierarchical model for air pollution](#stprocess)

`r vspace("30px")`

2\. [Implementation of a spatio-temporal process using `R-INLA`](#stinla)


---

name: stprocess
  
`r vspace("250px")`

.myblue[.center[.huge[
**Spatio-temporal processes + a space-time hierarchical model for air pollution**]]]



---

# Spatio-temporal processes

-  The concept of spatial process can be extended to the spatio-temporal case including a time dimension. The data are then defined by a process 
$\{y(s,t), (s,t) \in \mathcal D \subset \mathbb{R}^{2}\times \mathbb{R}\}$
and are observed at $n$ spatial locations and at $T$  time points. 

--

`r vspace("20px")`
- When spatio-temporal geostatistical data are considered, we need to define a valid .red[**spatio-temporal covariance function**] given by 
$$\mbox{Cov}\left(y(\bm s_i,t), y(\bm s_j,u)\right)=\mathcal {C}(y_{it},y_{ju})$$
--
`r vspace("20px")`

- If we assume .red[**stationarity in space and time**], the space-time covariance function can be written as a function of the spatial Euclidean distance $\Delta_{ij}=||\bm s_{i}-\bm s_{j}||$ and of the temporal lag $\Lambda_{tu}=|t-u|$ so that $\mbox{Cov}\left(y_{it}, y_{ju}\right)=\mathcal C(\Delta_{ij},\Lambda_{tu})$.

--
`r vspace("20px")`
-  If we assume .red[**separability**] the stationary space-time covariance function is decomposed into the product (or the sum) of a purely spatial and a purely temporal term: $$\mbox{Cov}\left(y_{it}, y_{ju}\right)=\mathcal{C}_1(\Delta_{ij})\mathcal{C}_2(\Lambda_{tu})$$

---

# Hierarchical spatio-temporal model for PM concentrations [1]

- We present a spatio-temporal model for fine air particulate matter (PM2.5; particles less than 2.5 micrometers in diameter) measured yearly in Spain.

`r vspace("20px")`

- The data have been collected over the years 2015 to 2017 from a set of sparse ground monitors.

`r vspace("20px")`

- The .red[objective] of our analysis is to predict expected air pollution concentrations at arbitrary space-time locations from the observed data. This will allow us to construct high-resolution maps of air pollution for each year.

`r vspace("20px")`

- The hierarchical spatio-temporal model that we use to reach this objective is a .red[separable model], where the space-time covariance structure can be decomposed into a spatial and a temporal term, i.e., the spatio-temporal covariance can be expressed as a Kronecker product of a spatial and a temporal covariance.

---

# Hierarchical spatio-temporal model for PM concentrations [2]


- We denote by $y_{it}$ the PM2.5 concentrations measured at site $\bm s_i$, with $i=1,\ldots,n$, and year $t=1,\ldots,T$. 

`r vspace("20px")`
- The following distribution is assumed for the observations:
$$\color{blue}{y_{it}\sim \text{Normal}( \eta_{it},\sigma^2_e)}$$
where $\sigma^{2}_{e}$ is the variance of the measurement error defined by a Gaussian white-noise process, both serially and spatially uncorrelated.

--

`r vspace("20px")`
- The linear predictor is given by
$$\color{blue}{\eta_{it} = b_0 +  \omega_{it}}$$
where $b_0$  is the intercept.

---

# Hierarchical spatio-temporal model for PM concentrations [3]

- The term $\omega_{it}$ refers to the **latent spatio-temporal process** (i.e. the true unobserved level of pollution) which changes in time with first order autoregressive dynamics and spatially correlated innovations:

$$\color{blue}{\omega_{it} = \rho\omega_{i(t-1)}+\xi_{it}}$$
with $t=2,\ldots,T$, $|\rho|<1$, $\omega_{i1}\sim\text{Normal}\left(0,\sigma^2/ (1-\rho^2)\right)$.

--

`r vspace("20px")`
- The term $\xi_{it}$ is a zero-mean **Gaussian field**, assumed to be **temporally independent** and characterized by the following spatio-temporal covariance function:

$$\text{Cov}\left(\xi_{it},\xi_{ju}\right) =\left\{
\begin{array}[c]{ccc}%
0 &  &  \text{if} \qquad t\neq u\\
\text{Cov}(\xi_i,\xi_j) && \text{if} \qquad t=u
\end{array}
\right.$$

for $i\neq j$, where $\text{Cov}(\xi_i,\xi_j)$ is given by Mat&eacute;rn spatial covariance function.

`r vspace("20px")`
- This model is characterized by a **separable spatio-temporal covariance** as it can be rewritten as the product of a purely spatial and a purely temporal covariance function (see `r Citet(bibfile,"cameletti2011")`).

---

# Hierarchical spatio-temporal model for PM concentrations [4]

- For each time point $\bm \xi_t \sim \text{Normal}(\bm 0, \bm\Sigma)$ and through the SPDE approach 
$$ \bm \xi_t \rightarrow \tilde{\bm \xi_t}\sim\text{Normal}(\bm 0,\bm Q^{-1}_S)$$
where the precision matrix $\bm Q_S$ comes from the SPDE representation. The matrix $\bm Q_S$  does not change in time - due to the serial independence hypothesis - and its dimension is given by the number of vertices of the domain triangulation.

--

`r vspace("20px")`
- The joint distribution of the $Tn$-dimensional GMRF $\bm\omega=(\bm\omega_1^\prime,\ldots,\bm\omega_T^\prime)^\prime$ is 
$$\bm\omega\sim\text{Normal}(\bm 0,\bm Q^{-1})$$
with $\bm Q=\bm Q_T \otimes \bm Q_S$, where $\otimes$ denotes the Kronecker product and $\bm Q_T$ is the $T$-dimensional precision matrix of the AR(1) process.

`r vspace("20px")`
- For the considered model the latent process is given by $\bm \theta=\{\bm \omega, b_0\}$ while the hyperparameter vector is $\psi=(\sigma^2_e, \rho, \sigma^2, r)$.

---

name: stinla
  
`r vspace("250px")`

.myblue[.center[.huge[
**Implementation of a spatio-temporal process using `R-INLA`**]]]


---

# Particle matter data

- The data used in this Session to demonstrate the modelling framework are **PM2.5 concentrations** measured at several monitoring stations in Spain over the years 2015 to 2017 from the European Environment Agency. This example is taken from `r Citet(bibfile,"moraga2019")`, chapter 10.


.panelset[
.panel[.panel-name[Data]
```{r, echo=F}
op <- options("width"=250) # large number to trick head, otherwise see next 
```

```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
library(tidyverse)
library(INLA)

df = read.csv("data/dataPM25.csv")
df = df[, c(
  "ReportingYear", "StationLocalId",
  "SamplingPoint_Longitude",
  "SamplingPoint_Latitude",
  "AQValue"
)]
names(df) = c("year", "id", "long", "lat", "value")
```
```{r, echo=F}
options(op) # set options back to default
```
]
.panel[.panel-name[Boundary]
.pull-left[
```{r, eval=TRUE, echo=TRUE, message=FALSE, warning=FALSE}
library(lwgeom)
library(raster)
library(sf)
m = getData(name = "GADM", 
    country = "Spain", level = 0) 
class(m)
```
```{r spainborder, eval=F}
library(inlabru)
ggplot()+
  gg(m) +
  coord_equal()
```
]
.pull-right[
```{r spainmap,ref.label="spainborder", echo=F, fig.height=1.9, out.width="100%"}
```
]
]
]

---

# Domain of the analysis [1]

- We are interested in predicting fine air particle in the main territory of Spain, and therefore we will remove the islands from the map. 

`r vspace("20px")`

- To do so we will keep the polygon of the map that has the largest area using the packages `sf` and `dplyr`. 

`r vspace("20px")`

- In detail, we will convert the object `m`, which is a `SpatialPolygonsDataFrame` object, to an `sf` object, and we will calculate the areas of the polygons of the object, and keep the polygon with the largest area.

`r vspace("20px")`

- Additionally, we will transform our map to an object with UTM projection to work with meters. To do this, we use the `st_transform()` function specifying the ESPG code of Spain (code 25830) which  corresponds to UTM zone 30 North.

```{r, echo=TRUE, warning=FALSE}
library(ggplot2)
m = m %>%
  st_as_sf() %>% #from sp to sf
  st_cast("POLYGON") %>% # convert to polygon
  mutate(area = st_area(.)) %>% # Extract geometric info (area)
  arrange(desc(area)) %>%
  slice(1) # slices the data by row index

m = m %>% st_transform(25830)
```

---

# Domain of the analysis [2]

.pull-left[
```{r spainonly, echo=TRUE, eval=F}
ggplot(m) + geom_sf() + 
  theme_bw() + coord_sf(datum = st_crs(m)) +
  labs(x = "", y = "") 
```
]
.pull-right[
```{r spainmap2,ref.label="spainonly", echo=F, eval=FALSE, fig.height=1.8, out.width="100%"}
```
`r include_fig("MapUTM.jpeg", width="60%",title="")`
]


---

# Data preparation

- We now project the data which uses geographical coordinates (longitude and latitude) to UTM projection. 

`r vspace("20px")`

- We create an `sf` object with the longitude and latitude values of the monitoring stations, and set the CRS to EPSG 4326 which corresponds to geographical coordinates. Then we use `st_transform()` to project the data to EPSG code 25830 and we include such coordinates in our PM2.5 data set

```{r, echo=TRUE}
# from lon lat to UTM
p = st_as_sf(data.frame(long = df$long, lat = df$lat),
              coords = c("long", "lat"))
st_crs(p) = st_crs(4326)
p = p %>% st_transform(25830)
df[, c("x", "y")] = st_coordinates(p)

# keep station in the main territory of Spain
ind = st_intersects(m, p)
df = df[ind[[1]], ]
head(df)[1:5,]
```

---

# Map of particle data 

.pull-left[
We plot PM2.5 concentrations measured at the monitoring stations by year

```{r mapPM_3years, echo=TRUE, eval=F}
library(tidyverse)
library(viridis)

ggplot(m) + geom_sf() + coord_sf(datum = NA) +
  geom_point(
  data = df, aes(x = x, y = y, color = value),
  size = 2) +
  labs(x = "", y = "") +
  scale_color_viridis() +
  facet_wrap(~year, ncol = 2, nrow = 2) +
  theme_bw()

```
]
.pull-right[
```{r mapPM_3years_out, echo=F, ref.label="mapPM_3years", fig.height=4, out.width="100%"}
```
]

---

# Create the mesh and the SPDE model

We obtain a triangulation of the domain using the function `inla.mesh.2d`

.pull-left[
```{r, echo=TRUE}
sc = 1/1000 ##scaling
coo = cbind(df$x, df$y)*sc

bnd = inla.nonconvex.hull(
  st_coordinates(m)[, 1:2]*sc)

mesh = inla.mesh.2d(
  loc = coo, boundary = bnd,
  max.edge = c(100000, 200000)*sc, 
  cutoff = 1000*sc)

```

Given the mesh, it is now possible to create the SPDE model using the `inla.spde2.matern` or `inla.spde2.matern` functions. 

```{r, echo=TRUE, eval=TRUE}
spde = inla.spde2.matern(mesh = mesh) 
spde$n.spde #n. of mesh vertices
```


]
.pull-right[
```{r mesh, echo=F, fig.dim=c(4.5, 4), out.width="100%"}
plot(mesh)
points(coo, col = "red")

```
]

---

# The index set

- The function `inla.spde.make.index`, generates vectors of indices for the spatial and temporal components of the model. We specify the name of the effect and the number of vertices in the SPDE model (`spde$n.spde`)

`r vspace("20px")`

```{r, echo=TRUE}
n_years = length(unique(df$year))

indexs = inla.spde.make.index("spatial.field",
             n.spde = spde$n.spde,
             n.group = n_years
)
names(indexs)
```

where:
`r vspace("20px")`
 – `spatial.field`: indices of the SPDE vertices repeated the number of times,
 `r vspace("20px")`
 – `spatial.field.group`: indices of the times repeated the number of mesh vertices,
 `r vspace("20px")`
 – `spatial.field.repl`: vector of 1s with length given by the number of mesh vertices times the number of times (`spde$n.spde*n_years`; because we have a
single replication of the spatial process at each time point).


---

# A matrix for the estimation part

- We construct an observation matrix that extracts the values of the spatio-temporal field at the measurement locations and time points used for the parameter estimation. This is the .red[projection matrix A], that is used to project the Gaussian random field (GRF) from the observations to the triangulation vertices

- The projection matrix is defined using the coordinates of the observed data. In order to construct a Kronecker product model in `R-INLA`, we use the
`group` feature in the `inla.spde.make.A`

`r vspace("20px")`

```{r, echo=TRUE}
group = df$year - min(df$year) + 1
A_est = inla.spde.make.A(mesh = mesh, loc = coo, group = group)
dim(A_est) 
```

This is a matrix equal in dimension to (number of observations) $\times$ (number of indices) of our basis functions in space and time:

```{r, echo=TRUE}
nrow(df)
length(indexs$spatial.field)
```

---

# Grid locations for predictions [1]

- We want to calculate predictions of the expected particle concentrations for the entire Spain. 

`r vspace("20px")`

- To do so, we need to create a spatial grid upon which we map the predictions. Here, we create an 50 $\times$ 50 grid. 


```{r, echo=TRUE, fig.width=5, fig.height=3}
# Grid construction
bb = st_bbox(m)
x = seq(bb$xmin - 1, bb$xmax + 1, length.out = 50)
y = seq(bb$ymin - 1, bb$ymax + 1, length.out = 50)
dp = as.matrix(expand.grid(x, y))
#plot(dp, asp = 1, xlab="", ylab="")
```

`r include_fig("grid1.jpg", width="43%",title="")`
---

# Grid locations for predictions [2]

- We intersect the created grid with the map of Spain, to keep only the locations that lie within that map 

```{r, echo=TRUE, fig.width=5, fig.height=3}
# keep only locations within borders of Spain
p = st_as_sf(data.frame(x = dp[, 1], y = dp[, 2]),
            coords = c("x", "y"))
st_crs(p) = st_crs(25830)
ind = st_intersects(m, p)
dp = dp[ind[[1]], ]
#plot(dp, asp = 1, xlab="", ylab="")
```

`r include_fig("grid2.jpg", width="43%",title="")`

---

# Grid for predictions & A matrix for the predicton part

- Finally, as we want to predict over three years, we bind three times the coordinates  at the prediction points, specifying the times as: time 1 for 2015, time 2 for 2016, and time 3 for 2017.

`r vspace("30px")`

```{r, echo=TRUE}
dp_final = rbind(cbind(dp, 1), cbind(dp, 2), cbind(dp, 3))
head(dp_final)
```

```{r, echo=TRUE}
# A matrix for predictions
coo_pred = dp_final[, 1:2]*sc
groupp = dp_final[, 3]
A_pred <- inla.spde.make.A(mesh = mesh, 
                           loc = coo_pred, # prediction locations
                           group = groupp) # time indices 
```
---

# Data stack preparation

To make predictions, we need to construct the .red[stacks for the estimation and prediction, then to join them]:

- First we build the estimation stack:
```{r, echo=TRUE}
stack_est <- inla.stack(
  tag = "est",
  data = list(y = df$value),
  A = list(1, A_est),
  effects = list(data.frame(b0 = rep(1, nrow(df))), indexs))
```

- Then, to predict within the `inla` fitting function, we need to create a stack also for prediction. Note that for the response in the prediction stack we will set it to $y=$NA:
```{r, echo=TRUE}
stack_pred = inla.stack(
  tag = "pred",
  data = list(y = NA),
  A = list(1, A_pred),
  effects = list(data.frame(b0 = rep(1, nrow(dp_final))), indexs))
```

- Lastly, we join the prediction and observed data stack together
```{r, echo=TRUE}
stack = inla.stack(stack_est, stack_pred)

```

---

# Define the formula

- We define the formula, specifying a PC prior for the temporal correlation parameter $\rho$ linked to the AR(1) model, such that $p(\rho>0=0.9)$

```{r, echo=TRUE, message=FALSE}
rho_hyper = list(theta = list(prior = "pccor1", param = c(0, 0.9)))

formula = y ~ -1 + b0 + f(spatial.field,
                          model = spde, group = spatial.field.group,
                          control.group = list(model = "ar1", hyper = rho_hyper))
```

Note that using the options `group` and `control.group` we specify that at each time point the spatial locations are linked by the `spde` model object, while across time the process evolves according to an AR(1) dynamics.


---

# Fit the space-time model!

```{r, echo=TRUE}
fit = inla(formula,
          data = inla.stack.data(stack, spde=spde),
          family = "gaussian",
          control.predictor = list(A = inla.stack.A(stack), compute = TRUE),
          control.compute = list(return.marginals.predictor = TRUE))

```

```{r echo=TRUE}
round(fit$summary.fixed[,c("mean","0.025quant","0.975quant")],3)
```

```{r, echo=TRUE}
round(fit$summary.hyperpar, 3)
```

---

# Marginal posterior distribution of the intercept $b0$

```{r, echo=TRUE}
modfix = fit$summary.fixed
modfix
```
```{r, echo=TRUE, eval=FALSE}
plot(fit$marginals.fix$b0,type ='l',xlab=expression(beta[0]),ylab="density")
abline(v = modfix[1, c(3, 5)], lty=2)
```
`r vspace("20px")`

`r include_fig("b0.png", width="43%")`

---

# Plot of the posterior distribution of the temporal correlation $\rho$ 

```{r echo=TRUE,eval=F}
# AR1 parameter    
plot(fit$marginals.hyperpar$`GroupRho for spatial.field`,
     type = 'l',xlab = expression(rho),ylab = "density", xlim=c(0.8,1))
```

`r vspace("20px")`

`r include_fig("rho.png", width="43%",title="")`

We can see that the AR(1) coefficient of the latent field, $\rho$, is large and most of the mass of the posterior distribution is close to 1.
---

# Plot of the variance and range parameters [1]

- We use the function `inla.spde2.result` to extract field and parameter values and distributions for the `inla.spde` SPDE effect from the INLA result object

```{r, echo=TRUE}
output.field = inla.spde2.result(inla = fit,
                                  name = "spatial.field",
                                  spde = spde,
                                  do.transf = TRUE)
```

- Then we plot the parameters of the spatial field 
```{r, echo=TRUE, eval=FALSE}
par(mfrow=c(1,2))    # set the plotting area into a 1*2 array

plot(output.field$marginals.variance.nominal[[1]],type = 'l',
     xlab = expression(sigma^2),ylab = "density")

plot(output.field$marginals.range.nominal[[1]],type = 'l',
     xlab = "spatial range",ylab = "density") 

```

---

# Plot of the variance and range parameters [2]

`r include_fig("Plot_Var_Range.jpeg", width="47%")`

These posterior distributions suggest that there is strong spatial and temporal dependencies in the data.
---

# Map of the predicted particle concentrations (and 95% CI) by year 

- To map the predicted concentrations of PM2.5, we need (i) to extract the indices to the prediction nodes and then (ii) to extract the posterior mean (and related 95%CI) of the response. To retrieve these predictions, we use `inla.stack.index()` to obtain the indices of the `stack` that correspond to `tag = "pred"`

```{r, echo=TRUE}
index = inla.stack.index(stack = stack, tag = "pred")$data

dp_final = data.frame(dp_final)
names(dp_final) = c("x", "y", "time")

dp_final$pred_mean = fit$summary.fitted.values[index, "mean"]
dp_final$pred_ll = fit$summary.fitted.values[index, "0.025quant"]
dp_final$pred_ul = fit$summary.fitted.values[index, "0.975quant"]

library(reshape2)
dpm = melt(dp_final, # melt dp_final into a long data frame 
            id.vars = c("x", "y", "time"),
            measure.vars = c("pred_mean", "pred_ll", "pred_ul"))
head(dpm)[1:3,]

```
---

# And finally the yearly maps for PM2.5 concentrations!
```{r, echo=TRUE, eval=FALSE}
# map of posterior means and associated uncertainty
ggplot(m) + geom_sf() + coord_sf(datum = NA) + 
  geom_tile(data = dpm, aes(x = x, y = y, fill = value)) + labs(x = "", y = "") +
  facet_wrap(variable ~ time) + scale_fill_viridis("PM2.5") + theme_bw()

```

`r include_fig("Maps_predictions.jpg", width="39%")`
---

# Model Validation

- An important aspect of Bayesian modelling regards the assessment of its plausibility and fit.

`r vspace("20px")`

-  We studied different techniques that can be use to select the best model for the data in analysis, such as the DIC or the WAIC.

`r vspace("20px")`

- In particular for point-level (geostatistical) spatial and spatio-temporal models, validation through the  evaluation on the predictive performance is extremely important.

`r vspace("20px")`

- In this last section of this lecture we explore an additional technique for the validation of our model(s), such as cross-validation.

---

# Idea about cross-validation

- Once we have fitted the model, we may want to know about the performance of the resulting spatial or spatio-temporal process model. This can be performed by cross-validation.

`r vspace("20px")`

- Cross-validation is a methodology used to evaluate model prediction performance by splitting up the data set into a .red[training sample] and a .red[validation sample], then fitting the model with the training sample and evaluating it with the validation sample.

`r vspace("20px")`

-  This methodology allow us to evaluate how well our model reproduces the real-world quantities.

---

# Cross-validation and $k$-fold cross-validation

- Specifically, to perform this validation assessment, we split the data set into two disjoint subsets. One of these subsets is used to learn the parameters and the other subset is our validation (or test) set used to estimate the mean test error.

`r vspace("20px")`

-  Dividing the data set into a fixed training set and a fixed validation set can be problematic if it results in a validation set being small.

`r vspace("20px")`

- In the situation of small data set, there are alternative procedures, which are based on the idea of repeating the training and validation (testing) computation on different randomly chosen subsets of the original data set.

`r vspace("20px")`

- The most common of these is the .blue[*k*-fold cross-validation], where a (random) partition of the data set is formed by splitting it into *k* nonoverlapping subsets. The results from the different partitions are then combined to produce single estimations of the error.
---

# Metrics used for evaluating prediction performance

- A number of criteria can be used to evaluate the predictive capability of competing models. 
`r vspace("20px")`
- Let's introduce first the notation:

  - $m$ is the total number of observations we want to validate

  - $y_i$ is the data indexed by $i$

  - $\hat{y}_i$ is the prediction value


- Some popular metrics are:
`r vspace("20px")`
  - .blue[Mean Squared Error]: $MSE=\frac{1}{m}\sum_{i=1}^{m}(\hat{y}_i-y_i)^2$
  `r vspace("20px")`
  - .blue[Root Mean Squared Error]: $RMSE=\sqrt{\frac{1}{m}\sum_{i=1}^{m}(\hat{y}_i-y_i)^2}$
  `r vspace("20px")`
  - .blue[Mean Absolute Error]: $MAE=\frac{1}{m}\sum_{i=1}^{m}|\hat{y}_i-y_i|$
  `r vspace("20px")`
  - .blue[Mean Absolute Percentage Error]: $MAPE=\frac{1}{m}\sum_{i=1}^{m}|(\hat{y}_i-y_i)/y_i|$
  `r vspace("20px")`
  - .blue[Bias]: $BIAS=(\hat{y}_i-y_i)$

---

# References

```{r refs, echo=FALSE, results="asis"}
PrintBibliography(bibfile,.opts=list(max.names=3))
```

