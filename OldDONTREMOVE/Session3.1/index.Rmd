---
title: "Session 3.1: Introduction to INLA and `R-INLA`"
params: 
   conference: ""
   location: ""
   date: ""
output:
   xaringan::moon_reader: 
    includes: 
       # This line adds a logo based on the format selected in the file 'assets/include_logo.html'
       in_header: "assets/latex_macros.html" 
       # NB: the actual options (eg placement of the logo and actual logo file) can be changed there
     #  after_body: "assets/insert-logo.html" # Monica commented this
    seal: false
    yolo: no
    lib_dir: libs
    nature:
      beforeInit: ["assets/remark-zoom.js","https://platform.twitter.com/widgets.js"]
      navigation:
        scroll: false # disable slide transitions by scrolling
      highlightStyle: github
      highlightLines: yes
      countIncrementalSlides: no
      ratio: '16:9'
      titleSlideClass:
      - center
      - middle
    self_contained: false 
    css:
    - "assets/beamer.css"
editor_options: 
  chunk_output_type: console
---

```{r global_options, echo = FALSE, include = FALSE}
options(width = 999)
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE,
                      cache = FALSE, tidy = FALSE, size = "small")
#options(htmltools.preserve.raw = FALSE)
# https://stackoverflow.com/questions/65766516/xaringan-presentation-not-displaying-html-widgets-even-when-knitting-provided-t
```

```{r echo=F, message=FALSE, warning=FALSE, comment=NA}
source("assets/setup.R")
library(INLA)
xaringanExtra::use_panelset()
bibfile=RefManageR::ReadBib("C:/Users/magb/Dropbox/Books/INLABook/ShortCourse/VIBASS/Session3.1/Biblio.bib",check = FALSE)
```

class: title-slide

# `r rmarkdown::metadata$title``r vspace("10px")` `r rmarkdown::metadata$subtitle`

## `r rmarkdown::metadata$author`

### `r rmarkdown::metadata$institute`    

### `r rmarkdown::metadata$params$conference`, `r rmarkdown::metadata$params$location` 

<!-- Can also separate the various components of the extra argument 'params', eg as in 
### `r paste(rmarkdown::metadata$params, collapse=", ")`
-->

`r ifelse(is.null(rmarkdown::metadata$params$date),format(Sys.Date(),"%e %B %Y"),rmarkdown::metadata$params$date)`


`r ifelse(is.null(rmarkdown::metadata$params$date),format(Sys.Date(),"%e %B %Y"),rmarkdown::metadata$params$date)`

`r vspace("20px")`

`r include_fig("LogoMRC.png")`

---

layout: true  

.my-footer[ 
.alignleft[ 
&nbsp; &copy; Marta Blangiardo | Monica Pirani
]
.aligncenter[
`r rmarkdown::metadata$params$short_title` 
]
.alignright[
`r rmarkdown::metadata$params$conference`, `r short_date` 
]
] 

```{css,echo=FALSE, eval=FALSE}
.red {
  color: red;
}
.blue {
  color: 0.14 0.34 0.55;
}

.content-box-blue { background-color: #F0F8FF; }

}
```
<style>
pre {
  overflow-x: auto;
}
pre code {
  word-wrap: normal;
  white-space: pre;
}
</style>

---

# Learning objectives

After this lecture you should be able to 
`r vspace("40px")`
- Present the class of latent Gaussian models     
`r vspace("40px")`
- Present the Laplace approximation and the INLA approach    
`r vspace("40px")`
- Use the basic functions of the `R-INLA` package     
`r vspace("40px")`

The topics treated in this lecture are presented in Chapter 4  of `r Citet(bibfile, "blangiardo2015spatial")`.

---

# Outline 

`r vspace("30px")`

1\. [MCMC to INLA](#mcmcINLA)

`r vspace("30px")`


2\. [Latent Gaussian models](#LGM)

`r vspace("30px")`

3\. [The INLA approach](#INLA)

`r vspace("30px")`

4\. [`R-INLA` package](#Package)

---

name: mcmcINLA
  
`r vspace("250px")`

.myblue[.center[.huge[
**MCMC to INLA**]]]

---

# From MCMC to INLA

- `Standard` MCMC sampler are generally easy-ish to program and are in fact implemented in readily available software

- MCMC methods are flexible and able to deal with virtually any type of data and model, but they  involve computationally- and time- intensive simulations to obtain the posterior distribution for the parameters. For this reason the complexity of the model and the database dimension often remain fundamental issues. 

--


- The INLA algorithm proposed by `r Citet(bibfile,"INLA:09")` is a .red[*deterministic*] algorithm for Bayesian inference and it represents an alternative to MCMC which is instead a simulation based algorithm.

--


- The INLA algorithm is designed for the class of .red[*latent Gaussian models*] and compared to MCMC it provides (as) accurate results in a shorter time.

--


- INLA has become very popular among statisticians and applied researchers  and in the past few years the number of papers reporting usage and extensions of the INLA method has increased considerably.

`r vspace("-20px")`

`r include_fig("INLA_citations.png",width="60%",title="")`


---

# INLA website and community

- The website contains source code, examples, papers and reports discussing the theory and applications of INLA.

- There is also a discussion forum where users can post queries and requests of help.

- Almost each year there is an INLA-related scientific meeting.

`r include_fig("INLA_website.png", width="50%", title="")`


.center[[INLA website](https://www.r-inla.org)]

---

name: LGM
  
`r vspace("250px")`

.myblue[.center[.huge[
**Latent Gaussian models**]]]

---

# Latent Gaussian models (LGMs)
- The general problem of (parametric) inference is posited by assuming a probability model for the observed data $\boldsymbol y=\left(y_1,\ldots, y_n\right)$, as a function of some relevant parameters

`r vspace("-10px")`

.myblue[$$\boldsymbol{y} \mid \boldsymbol{\theta},\boldsymbol\psi \sim p(\boldsymbol{y} \mid \boldsymbol{\theta},\boldsymbol\psi) = \prod_{i=1}^n p(y_i \mid \boldsymbol{\theta},\boldsymbol\psi)$$]
--

`r vspace("-10px")`

- Often (in fact for a surprisingly large range of models!), we can assume that the parameters are described by a .red[**Gaussian Markov Random Field** (GMRF)]


.myblue[
$$\boldsymbol{\theta} \mid \boldsymbol\psi \sim \text{Normal}(\boldsymbol 0,\boldsymbol{Q^{-1}}(\boldsymbol\psi))$$]

.myblue[
$$\theta_i \perp\!\!\!\perp \theta_j \mid \boldsymbol\theta_{-i,j}   \Longleftrightarrow Q_{ij}(\boldsymbol\psi)=0$$
]

`r vspace("-20px")`

where 
- The precision matrix $\bm Q$ depends on some hyperparameters $\bm\psi$ .
- The notation $-i,j$ indicates all the other elements of the parameters vector, excluding elements $i$ and $j$
- The components of $\bm \theta$ are supposed to be *conditionally independent* with the consequence that $\bm Q$ is a sparse precision matrix.

- This kind of models is often referred to as .olive[**Latent Gaussian models**].

---

# LGMs as a general framework

- In general

.myblue[
\begin{align*}
\bm y \mid \bm \theta,\bm\psi  &\sim  \prod_i p(y_i\mid\bm\theta,\bm\psi)\class{black}{\mbox{  (Data model)}}\\
\bm\theta \mid \bm\psi &\sim  p(\bm\theta\mid\bm\psi) = \mbox{Normal}(0,\bm Q^{-1}(\bm\psi)) \class{black}{\mbox{  (Latent Gaussian Field)}}\\
\bm\psi &\sim p(\bm\psi) \class{black}{\mbox{ (Hyperprior)}}
\end{align*}
]

--

`r vspace("30px")`

- The dimension of $\class{myblue}{\bm\theta}$ can be very large (eg 10 $^2$-10 $^5$).

`r vspace("10px")`

- Conversely,  the dimension of $\class{myblue}{\bm\psi}$ must be relatively small (less than 20 is recommended) to avoid an exponential increase in the computational costs of the model.

---

# LGMs as a general framework

- A very general way of specifying the problem is specifying a distribution for $\class{myblue}{y_i}$ characterized by a parameter  $\class{myblue}{\phi_i}$ (usually the mean) defined as a function of a structured additive predictor $\class{myblue}{\eta_i}$, defined on a suitable scale, such that  $\class{myblue}{g(\phi_i)=\eta_i}$ (e.g. logistic for binomial data):

.myblue[
$$\eta_i = \beta_0 + \sum_{m=1}^M \beta_m x_{mi} + \sum_{l=1}^L f_l(z_{li})$$
]

where
- $\beta_0$ is the intercept; 
- $\bm\beta=\{\beta_1,\ldots,\beta_M\}$ quantify the effect of the covariates $\bm{x}=(\bm{x_1},\ldots, \bm{x_M})$ on the response; 
- $\bm{f}=\{f_1(\cdot),\ldots,f_L(\cdot)\}$ is a set of functions defined in terms of some covariates $\bm{z}=(\bm z_1,\ldots, \bm z_L)$

and then assume 

$$\class{red}{\bm\theta=\{\beta_0,\bm \beta,\bm f \} \sim \mbox{Normal}(\bm 0,\bm Q^{-1}(\bm\psi)) = \mbox{GMRF}(\bm\psi)}$$

--

- **NB**: This of course implies some form of Normally-distributed marginals for $\beta_0,\bm{\beta}$ and $\bm{f}$

---

# LGMs as a general framework --- examples

`r vspace("-10px")`

Upon varying the form of the functions $\class{myblue}{f_l(\cdot)}$, this formulation can accommodate a wide range of models (see `r Citet(bibfile,"Martinsetal:2012")` for a review)


- Standard regression

  - $\class{myblue}{f_l(\cdot) = \text{NULL}}$

--

- Hierarchical models

  - $\class{myblue}{f_l(\cdot) \sim \mbox{Normal}(0,\sigma^2_f)}$ (Exchangeable)    
   $\class{myblue}{\sigma^2_f\mid \bm\psi \sim}$ some common distribution

--

- Spatial and spatio-temporal models 

  - Areal data: $\class{myblue}{f_1(\cdot) \sim \mbox{CAR}}$ &nbsp; (Spatially structured effects)    
  $\class{myblue}{f_2(\cdot) \sim \mbox{Normal}(0,\sigma^2_{f_2})}$ &nbsp; (Unstructured residual)
  
  - Geostatistical data: $\class{myblue}{f(\cdot) \sim \mbox{Gaussian field}}$
  - Temporal component: $\class{myblue}{f(\cdot) \sim \mbox{RW}}$

--

- Spline smoothing

  - $\class{myblue}{f_l(\cdot) \sim \mbox{AR}(\phi,\sigma^2_\varepsilon)}$

--

- Survival models, logGaussian Cox Processes, etc.

<!-- # MCMC and LGMs -->

<!-- - (Standard) MCMC methods can perform poorly when applied to (non-trivial) LGMs. This is due to several factors -->
<!-- `r vspace("20px")` -->

<!--   - The components of the latent Gaussian field $\class{myblue}{\bm{\theta}}$ tend to be highly correlated, thus impacting on convergence and autocorrelation -->
<!--   `r vspace("20px")` -->
<!--   - Especially when the number of observations is large, $\class{myblue}{\bm{\theta}}$ and $\class{myblue}{\bm\psi}$ also tend to be highly correlated -->
<!--   `r vspace("20px")` -->
<!--   - Time to run can be very long -->


<!-- - Blocking and overparameterisation can **alleviate**, but rarely eliminate the problem -->


---

name: INLA

`r vspace("250px")`

.myblue[.center[.huge[
**The INLA approach**]]]


---

# Integrated Nested Laplace Approximation (INLA)

- The first *ingredient* of the INLA approach is the definition of conditional probability, which holds for any pair of variables $(x,z)$. 

Technically, provided $p(z)>0$

$$\class{myblue}{p(x\mid z) =: \frac{p(x,z)}{p(z)} \rightarrow p(x,z)=p(x\mid z)p(z)}$$

$\class{myblue}{p(x\mid z)}$ can be re-written as

$$\class{myblue}{p(z) = \frac{p(x,z)}{p(x\mid z)}}$$

--

- In particular, a conditional version can be obtained further considering a third variable $w$ as

$$\class{myblue}{p(z\mid w) = \frac{p(x,z\mid w)}{p(x\mid z,w)}}$$

which is particularly relevant to the Bayesian case.


---

# Integrated Nested Laplace Approximation (INLA)

- The second *ingredient* is .red[**Laplace approximation**]

--

`r vspace("-20px")`

- Main idea: approximate $\class{myblue}{\log f(x)}$ using a quadratic function by means of a Taylor's series expansion around the mode $x^*=\arg\!\max_x \log f(x)$

`r vspace("-20px")`
.myblue[
\begin{align*}
\log f(x) & \approx  \log f(x^*) +(x-x^*)\left.\frac{\partial \log f(x)}{\partial x}\right|_{x=x^*}+\frac{(x-x^*)^2}{2}\left.\frac{\partial^2 \log f(x)}{\partial x^2}\right|_{x=x^*}\\
& =\log f(x^*) +\frac{(x-x^*)^2}{2}\left.\frac{\partial^2 \log f(x)}{\partial x^2}\right|_{x=x^*}  \qquad\class{black}{\left(\text{since}\left.\frac{\partial \log f(x)}{\partial x}\right|_{x=x^*}=0\right)}
\end{align*}
]

--

`r vspace("-20px")`
- Setting $\class{myblue}{{\sigma^2}^*=-1/\left.\frac{\partial^2 \log f(x)}{\partial x^2}\right|_{x=x^*}}$ we can re-write 


$$\class{myblue}{\log f(x) \approx \log f(x^*) - \frac{1}{2  {\sigma^2}^*}(x-x^*)^2}$$


or equivalently 

.small[
$$\class{myblue}{\int f(x)dx = \int \exp[\log f(x)]dx \approx f(x^\star) \int \exp\left[ -\frac{(x- x^*)^2}{ 2  {\sigma^2}^*}\right] dx}$$ 
]

--

- Thus, under LA, $\class{red}{f(x)\approx \mbox{Normal}(x^*, {\sigma^2}^*)}$.

---

#Laplace approximation --- example

- Consider a $\chi^2$ distribution con $k$ degrees of freedom: $\class{red}{f(x) =  \frac{x^{\frac{k}{2}-1}e^{\frac{-x}{2}}}{2^{k/2}\Gamma(k/2)}}$

--

1\. $\class{myblue}{l(x) = \log f(x) = \left( \frac{k}{2}-1 \right)\log x - \frac{x}{2}+\mbox{constant}}$

--

2\. $\class{myblue}{l'(x)  = \frac{\partial \log f(x)}{\partial x} = \left( \frac{k}{2}-1 \right)x^{-1}-\frac{1}{2}}$

--

3\. $\class{myblue}{l''(x) = \frac{\partial^2 \log f(x)}{\partial x^2} = -\left( \frac{k}{2}-1 \right)x^{-2}}$

--

- Then

  - Solving $\class{myblue}{l'(x) = 0}$ we find the mode: $\color{olive}{x^*=k-2}$
  - Evaluating $\class{myblue}{-\frac{1}{l''(x)}}$ at the mode gives $\color{olive}{\sigma^{2^*}=2(k-2)}$

--

- Consequently, we can approximate $f(x)$ as
$$\class{myblue}{f(x) \approx  \mbox{Normal}(k-2,2(k-2))}$$

---

# Laplace approximation --- example

.panelset[
.panel[.panel-name[Fig 1]
```{r fig2, echo=FALSE, out.width="50%", opts=list(width="50%")}
# This would make a plot of 100000 random draws for a Normal(0,1) 
plot(density(rchisq(100000,6)),axes=T, main="",ylab="",xlab="",xlim=c(0,20),ylim=c(0,0.15))
lines(density(rnorm(100000,4,sqrt(8))),lty=2)
legend(15,0.15,lwd=0.5,legend=expression(paste(chi^2,"(6)")),bty="n")
legend(15,0.14,lwd=0.5,lty=2,legend=expression(paste(Normal,"(4,8)")),bty="n")
```
]

.panel[.panel-name[Fig 2]
```{r fig3, echo=FALSE, out.width="50%", opts=list(width="50%")}
# This would make a plot of 100000 random draws for a Normal(0,1) 
plot(density(rchisq(100000,10)),axes=T, main="",ylab="",xlab="",xlim=c(0,20),ylim=c(0,0.12))
lines(density(rnorm(100000,8,sqrt(16))),lty=2)
legend(15,0.12,lwd=0.5,legend=expression(paste(chi^2,"(10)")),bty="n")
legend(15,0.11,lwd=0.5,lty=2,legend=expression(paste(Normal,"(8,16)")),bty="n")
```
]

.panel[.panel-name[Fig 3]
```{r fig4, echo=FALSE, out.width="50%", opts=list(width="50%")}
# This would make a plot of 100000 random draws for a Normal(0,1) 
plot(density(rchisq(100000,20)),axes=T, main="",ylab="",xlab="",xlim=c(0,40),ylim=c(0,0.07))
lines(density(rnorm(100000,18,sqrt(36))),lty=2)
legend(30,0.068,lwd=0.5,legend=expression(paste(chi^2,"(20)")),bty="n")
legend(30,0.063,lwd=0.5,lty=2,legend=expression(paste(Normal,"(18,36)")),bty="n")
```
]
]


---

#Laplace approximation --- example


- Consider a Gamma distribution: $f(x)=\frac{b^a}{\Gamma(a)}\exp\left(-bx\right)x^{a-1}\qquad x,a,b>0.$

--

1\. $\class{myblue}{\log f(x)=(a-1)\log x -bx +\text{constant}}$

--

2\. $\class{myblue}{l'(x)=\frac{\partial \log f(x)}{\partial x}=\frac{a-1}{x}-b}$

--

3\. $\class{myblue}{l''(x)=\frac{\partial^2 \log f(x)}{\partial x^2}=-\frac{a-1}{x^2}}$

--

- Then

  - Solving $\class{myblue}{l'(x) = 0}$ we find the mode: $\color{olive}{\frac{a-1}{b}}$
  - Evaluating $\class{myblue}{-\frac{1}{l''(x)}}$ at the mode gives $\color{olive}{\frac{a-1}{b^2}}$

--

- Consequently, we can approximate $f(x)$ as
$$\class{myblue}{Gamma(a,b) \approx  \mbox{Normal}(x^* = \frac{a-1}{b},\sigma^{2*}=\frac{a-1}{b^2})}$$

---

# Laplace approximation --- example

.panelset[
.panel[.panel-name[Fig1]
```{r fig1_1, echo=FALSE, out.width="50%", opts=list(width="50%")}
a=3
b=3
mean<-(a-1)/b
sqrt=sqrt((a-1)/b^2)
xx<-seq(0,20,l=100)
plot(xx,dgamma(xx,shape=a,rate=b),axes=T,t="l",main="",ylab="",xlab="",xlim=c(0,5),ylim=c(0,1))
lines(xx,dnorm(xx, mean, sqrt),lty=2)

legend(3,0.8,lwd=0.5,legend=expression(paste("Gamma(3,3)")),bty="n")
legend(3,0.7,lwd=0.5,lty=2,legend=expression(paste(Normal,"(0.66,0.22)")),bty="n")
```
]
.panel[.panel-name[Fig 2]
```{r fig2_1, echo=FALSE, out.width="50%", opts=list(width="50%")}
# This would make a plot of 100000 random draws for a Normal(0,1) 
a=5
b=1
mean<-(a-1)/b
sqrt=sqrt((a-1)/b^2)
xx<-seq(0,20,l=100)
plot(xx,dgamma(xx,shape=a,rate=b),axes=T,t="l",main="",ylab="",xlab="",xlim=c(0,20),ylim=c(0,0.30))
lines(xx,dnorm(xx, mean, sqrt),lty=2)

legend(10,0.3,lwd=0.5,legend=expression(paste("Gamma(5,1)")),bty="n")
legend(10,0.28,lwd=0.5,lty=2,legend=expression(paste(Normal,"(4,4)")),bty="n")
```
]

.panel[.panel-name[Fig 3]
```{r fig3_1, echo=FALSE, out.width="50%", opts=list(width="50%")}
# This would make a plot of 100000 random draws for a Normal(0,1) 
a=10
b=0.5
mean<-(a-1)/b
sqrt=sqrt((a-1)/b^2)
xx<-seq(0,50,l=100)
plot(xx,dgamma(xx,shape=a,rate=b),axes=T,t="l",main="",ylab="",xlab="",xlim=c(0,50),ylim=c(0,0.30))
lines(xx,dnorm(xx, mean, sqrt),lty=2)

legend(10,0.3,lwd=0.5,legend=expression(paste("Gamma(5,1)")),bty="n")
legend(10,0.28,lwd=0.5,lty=2,legend=expression(paste(Normal,"(18,36)")),bty="n")
```
]
]


---

# Integrated Nested Laplace Approximation (INLA)
**Objective of Bayesian estimation**

- In a Bayesian LGM, the required distributions are 

$$\class{myblue}{p(\theta_i\mid\boldsymbol{y})} \class{black}{=  \int p(\theta_i,\boldsymbol\psi\mid \boldsymbol{y}) d\boldsymbol\psi =  \int} \class{red}{p(\boldsymbol\psi\mid\boldsymbol{y})} \class{orange}{p(\theta_i \mid \boldsymbol\psi,\boldsymbol{y})}\class{black}{d\boldsymbol\psi}$$
$$\class{myblue}{p(\psi_k\mid\boldsymbol{y})} \class{black}{ = \int}\class{red}{p(\boldsymbol\psi\mid\boldsymbol{y})}\class{black}{d\boldsymbol\psi_{-k}}$$

--

- Thus we need to estimate: 

  1\. $\class{red}{p(\boldsymbol\psi\mid\boldsymbol{y})}$, from which also all the relevant marginals $\class{myblue}{p(\psi_k\mid\boldsymbol{y})}$ can be obtained; 

--

  2\. $\color{orange}{p(\theta_i \mid \boldsymbol\psi,\boldsymbol{y})}$, which is needed to compute the marginal posterior for the parameters

---

# Integrated Nested Laplace Approximation (INLA)

1\. can be easily estimated as

$$\class{red}{p(\boldsymbol\psi\mid\boldsymbol{y})} \class{black}{= \frac{p(\boldsymbol\theta,\boldsymbol\psi\mid\boldsymbol{y})}{p(\boldsymbol\theta\mid\boldsymbol\psi,\boldsymbol{y})}} \text{ (Recall slide 17)}$$

---

count:false

# Integrated Nested Laplace Approximation (INLA)

1\. can be easily estimated as

\begin{align}
\class{red}{p(\boldsymbol\psi\mid\boldsymbol{y})} & \class{black}{= \frac{p(\boldsymbol\theta,\boldsymbol\psi\mid\boldsymbol{y})}{p(\boldsymbol\theta\mid\boldsymbol\psi,\boldsymbol{y})}}\\
& =\frac{p(\boldsymbol{y}\mid\boldsymbol\theta,\boldsymbol\psi)p(\boldsymbol\theta,\boldsymbol\psi)}{p(\boldsymbol{y})}\frac{1}{p(\boldsymbol\theta\mid\boldsymbol\psi,\boldsymbol{y})} \text{ (Bayes' theorem)}
\end{align}

---

count:false

# Integrated Nested Laplace Approximation (INLA)

1\. can be easily estimated as

\begin{align}
\class{red}{p(\boldsymbol\psi\mid\boldsymbol{y})} & \class{black}{= \frac{p(\boldsymbol\theta,\boldsymbol\psi\mid\boldsymbol{y})}{p(\boldsymbol\theta\mid\boldsymbol\psi,\boldsymbol{y})}}\\
& =\frac{p(\boldsymbol{y}\mid\boldsymbol\theta,\boldsymbol\psi)p(\boldsymbol\theta,\boldsymbol\psi)}{p(\boldsymbol{y})}\frac{1}{p(\boldsymbol\theta\mid\boldsymbol\psi,\boldsymbol{y})}\\
&=\frac{p(\boldsymbol{y}\mid\boldsymbol\theta, \boldsymbol \psi)p(\boldsymbol\theta\mid\boldsymbol\psi)p(\boldsymbol\psi)}{p(\boldsymbol{y})}\frac{1}{p(\boldsymbol\theta\mid\boldsymbol\psi,\boldsymbol{y})} \text{ (Conditional probability)}
\end{align}

---

count:false

# Integrated Nested Laplace Approximation (INLA)

1\. can be easily estimated as

\begin{align}
  \class{red}{p(\boldsymbol\psi\mid\boldsymbol{y})} & \class{black}{= \frac{p(\boldsymbol\theta,\boldsymbol\psi\mid\boldsymbol{y})}{p(\boldsymbol\theta\mid\boldsymbol\psi,\boldsymbol{y})}}\\
  & =\frac{p(\boldsymbol{y}\mid\boldsymbol\theta,\boldsymbol\psi)p(\boldsymbol\theta,\boldsymbol\psi)}{p(\boldsymbol{y})}\frac{1}{p(\boldsymbol\theta\mid\boldsymbol\psi,\boldsymbol{y})}\\
&=\frac{p(\boldsymbol{y}\mid\boldsymbol\theta, \boldsymbol \psi)p(\boldsymbol\theta\mid\boldsymbol\psi)p(\boldsymbol\psi)}{p(\boldsymbol{y})}\frac{1}{p(\boldsymbol\theta\mid\boldsymbol\psi,\boldsymbol{y})}\\ 
& \propto \frac{p(\boldsymbol\psi)p(\boldsymbol\theta\mid\boldsymbol\psi)p(\boldsymbol{y}\mid\boldsymbol\theta,\boldsymbol \psi)}{p(\boldsymbol\theta\mid\boldsymbol\psi,\boldsymbol{y})}
\end{align}

---

count:false

# Integrated Nested Laplace Approximation (INLA)

1\. can be easily estimated as

\begin{align}
\class{red}{p(\boldsymbol\psi\mid\boldsymbol{y})} & \class{black}{= \frac{p(\boldsymbol\theta,\boldsymbol\psi\mid\boldsymbol{y})}{p(\boldsymbol\theta\mid\boldsymbol\psi,\boldsymbol{y})}}\\
& =\frac{p(\boldsymbol{y}\mid\boldsymbol\theta,\boldsymbol\psi)p(\boldsymbol\theta,\boldsymbol\psi)}{p(\boldsymbol{y})}\frac{1}{p(\boldsymbol\theta\mid\boldsymbol\psi,\boldsymbol{y})}\\
&=\frac{p(\boldsymbol{y}\mid\boldsymbol\theta, \boldsymbol \psi)p(\boldsymbol\theta\mid\boldsymbol\psi)p(\boldsymbol\psi)}{p(\boldsymbol{y})}\frac{1}{p(\boldsymbol\theta\mid\boldsymbol\psi,\boldsymbol{y})}\\ 
& \propto \frac{p(\boldsymbol\psi)p(\boldsymbol\theta\mid\boldsymbol\psi)p(\boldsymbol{y}\mid\boldsymbol\theta,\boldsymbol \psi)}{p(\boldsymbol\theta\mid\boldsymbol\psi,\boldsymbol{y})}\\
& \approx \left. \frac{p(\boldsymbol\psi)p(\boldsymbol\theta\mid\boldsymbol\psi)p(\boldsymbol{y}\mid\boldsymbol\theta)}{\tilde{p}(\boldsymbol\theta\mid\boldsymbol\psi,\boldsymbol{y})}\right |_{\boldsymbol\theta={\boldsymbol\theta}^*(\boldsymbol\psi)} =: \class{red}{\tilde{p}(\boldsymbol\psi\mid\boldsymbol{y})}
\end{align}

where 

- $\tilde{p}(\boldsymbol\theta\mid\boldsymbol\psi,\boldsymbol{y})$ is the Gaussian approximation given by the Laplace method  of $p(\boldsymbol\theta\mid\boldsymbol\psi,\boldsymbol{y})$ 
- $\boldsymbol\theta={\boldsymbol\theta}^*(\boldsymbol\psi)$ is its mode for a given $\boldsymbol \psi$

---

# Integrated Nested Laplace Approximation (INLA)

2\. is slightly more complex, because in general there will be more elements in $\boldsymbol\theta$ than there are in $\boldsymbol\psi$ and thus this computation is more expensive.

--
`r vspace("20px")`

.olive[**Gaussian Approximation**]: one easy and cheap possibility is to start from $\tilde{p}(\boldsymbol\theta\mid\boldsymbol\psi,\boldsymbol{y})$ and approximate the density of $\class{orange}{\theta_i\mid\boldsymbol\psi,\boldsymbol{y}}$,  with the Gaussian marginals. While this is very fast, the approximation is generally not very good.

--
`r vspace("20px")`
.olive[**Full Laplace Approximation**]: alternatively, we can write $\boldsymbol\theta=\{\theta_i,\boldsymbol\theta_{-i}\}$, use the definition of conditional probability and again Laplace approximation to obtain $\class{orange}{\tilde{p}(\theta_i\mid\boldsymbol\psi,\boldsymbol{y})}$
(very computationally expensive)

--
`r vspace("20px")`
The most efficient algorithm is the .olive[**Simplified Laplace Approximation**]:
- Based on a Taylor's series expansion of the Laplace approximation $\class{orange}{\tilde{p}(\theta_i\mid\boldsymbol\psi,\boldsymbol{y})}$.
- The accuracy of this approximation is sufficient in many applied cases and that the computing time is considerably shorter, it is the standard option.

---



<!--
count:false

# Integrated Nested Laplace Approximation (INLA)

2\. is slightly more complex, because in general there will be more elements in $\boldsymbol\theta$ than there are in $\boldsymbol\psi$ and thus this computation is more expensive.

<!--
.olive[**Gaussian Approximation**]: one easy and cheap possibility is to start from $\tilde{p}(\boldsymbol\theta\mid\boldsymbol\psi,\boldsymbol{y})$ and approximate the density of $\class{orange}{\theta_i\mid\boldsymbol\psi,\boldsymbol{y}}$,  with the Gaussian marginal derived from $\tilde{p}(\boldsymbol\theta\mid\boldsymbol\psi,\boldsymbol{y})$ (using GMRF theory to derive the marginal variances). While this is very fast, the approximation is generally not very good.

.olive[**Full Laplace Approximation**]: alternatively, we can write $\boldsymbol\theta=\{\theta_i,\boldsymbol\theta_{-i}\}$, use the definition of conditional probability and again Laplace approximation to obtain

\begin{align*}
\class{orange}{p(\theta_i \mid\boldsymbol\psi,\boldsymbol{y})} & \class{black}{= \frac{p\left((\theta_i,\boldsymbol\theta_{-i})\mid \boldsymbol\psi,\boldsymbol{y}\right)}{p(\boldsymbol\theta_{-i}\mid\theta_i,\boldsymbol\psi,\boldsymbol{y})} = \frac{p\left((\theta_i,\boldsymbol\theta_{-i}), \boldsymbol\psi\mid\boldsymbol{y}\right)}{p(\boldsymbol\psi\mid \boldsymbol{y})} \frac{1}{p(\boldsymbol\theta_{-i}\mid\theta_i,\boldsymbol\psi,\boldsymbol{y})}}
\end{align*}
-->


<!--
count:false

# Integrated Nested Laplace Approximation (INLA)

2\. is slightly more complex, because in general there will be more elements in $\boldsymbol\theta$ than there are in $\boldsymbol\psi$ and thus this computation is more expensive.

.olive[**Gaussian Approximation**]: one easy and cheap possibility is to start from $\tilde{p}(\boldsymbol\theta\mid\boldsymbol\psi,\boldsymbol{y})$ and approximate the density of $\class{orange}{\theta_i\mid\boldsymbol\psi,\boldsymbol{y}}$,  with the Gaussian marginal derived from $\tilde{p}(\boldsymbol\theta\mid\boldsymbol\psi,\boldsymbol{y})$ (using GMRF theory to derive the marginal variances). While this is very fast, the approximation is generally not very good.

.olive[**Full Laplace Approximation**]: alternatively, we can write $\boldsymbol\theta=\{\theta_i,\boldsymbol\theta_{-i}\}$, use the definition of conditional probability and again Laplace approximation to obtain

\begin{align*}
\class{orange}{p(\theta_i \mid\boldsymbol\psi,\boldsymbol{y})} & \class{black}{= \frac{p\left((\theta_i,\boldsymbol\theta_{-i})\mid \boldsymbol\psi,\boldsymbol{y}\right)}{p(\boldsymbol\theta_{-i}\mid\theta_i,\boldsymbol\psi,\boldsymbol{y})} = \frac{p\left((\theta_i,\boldsymbol\theta_{-i}), \boldsymbol\psi\mid\boldsymbol{y}\right)}{p(\boldsymbol\psi\mid \boldsymbol{y})} \frac{1}{p(\boldsymbol\theta_{-i}\mid\theta_i,\boldsymbol\psi,\boldsymbol{y})}}\\
& \propto \frac{p\left(\boldsymbol\theta, \boldsymbol\psi\mid\boldsymbol{y}\right)}{p(\boldsymbol\theta_{-i}\mid\theta_i,\boldsymbol\psi,\boldsymbol{y})}
\end{align*}

-->


<!--
count:false

# Integrated Nested Laplace Approximation (INLA)

2\. is slightly more complex, because in general there will be more elements in $\boldsymbol\theta$ than there are in $\boldsymbol\psi$ and thus this computation is more expensive.

.olive[**Gaussian Approximation**]: one easy and cheap possibility is to start from $\tilde{p}(\boldsymbol\theta\mid\boldsymbol\psi,\boldsymbol{y})$ and approximate the density of $\class{orange}{\theta_i\mid\boldsymbol\psi,\boldsymbol{y}}$,  with the Gaussian marginal derived from $\tilde{p}(\boldsymbol\theta\mid\boldsymbol\psi,\boldsymbol{y})$ (using GMRF theory to derive the marginal variances). While this is very fast, the approximation is generally not very good.

.olive[**Full Laplace Approximation**]: alternatively, we can write $\boldsymbol\theta=\{\theta_i,\boldsymbol\theta_{-i}\}$, use the definition of conditional probability and again Laplace approximation to obtain

\begin{align*}
\class{orange}{p(\theta_i \mid\boldsymbol\psi,\boldsymbol{y})} & \class{black}{= \frac{p\left((\theta_i,\boldsymbol\theta_{-i})\mid \boldsymbol\psi,\boldsymbol{y}\right)}{p(\boldsymbol\theta_{-i}\mid\theta_i,\boldsymbol\psi,\boldsymbol{y})} = \frac{p\left((\theta_i,\boldsymbol\theta_{-i}), \boldsymbol\psi\mid\boldsymbol{y}\right)}{p(\boldsymbol\psi\mid \boldsymbol{y})} \frac{1}{p(\boldsymbol\theta_{-i}\mid\theta_i,\boldsymbol\psi,\boldsymbol{y})}}\\
& \propto \frac{p\left(\boldsymbol\theta, \boldsymbol\psi\mid\boldsymbol{y}\right)}{p(\boldsymbol\theta_{-i}\mid\theta_i,\boldsymbol\psi,\boldsymbol{y})} \propto \frac{p(\boldsymbol\psi)p(\boldsymbol\theta\mid\boldsymbol\psi)p(\boldsymbol{y}\mid\boldsymbol\theta,\boldsymbol \psi)}{p(\boldsymbol\theta_{-i}\mid\theta_i,\boldsymbol\psi,\boldsymbol{y})}
\end{align*}

-->


<!--

count:false

# Integrated Nested Laplace Approximation (INLA)

2\. is slightly more complex, because in general there will be more elements in $\boldsymbol\theta$ than there are in $\boldsymbol\psi$ and thus this computation is more expensive.

.olive[**Gaussian Approximation**]: one easy and cheap possibility is to start from $\tilde{p}(\boldsymbol\theta\mid\boldsymbol\psi,\boldsymbol{y})$ and approximate the density of $\class{orange}{\theta_i\mid\boldsymbol\psi,\boldsymbol{y}}$,  with the Gaussian marginal derived from $\tilde{p}(\boldsymbol\theta\mid\boldsymbol\psi,\boldsymbol{y})$ (using GMRF theory to derive the marginal variances). While this is very fast, the approximation is generally not very good.

.olive[**Full Laplace Approximation**]: alternatively, we can write $\boldsymbol\theta=\{\theta_i,\boldsymbol\theta_{-i}\}$, use the definition of conditional probability and again Laplace approximation to obtain

\begin{align*}
\class{orange}{p(\theta_i \mid\boldsymbol\psi,\boldsymbol{y})} & \class{black}{= \frac{p\left((\theta_i,\boldsymbol\theta_{-i})\mid \boldsymbol\psi,\boldsymbol{y}\right)}{p(\boldsymbol\theta_{-i}\mid\theta_i,\boldsymbol\psi,\boldsymbol{y})} = \frac{p\left((\theta_i,\boldsymbol\theta_{-i}), \boldsymbol\psi\mid\boldsymbol{y}\right)}{p(\boldsymbol\psi\mid \boldsymbol{y})} \frac{1}{p(\boldsymbol\theta_{-i}\mid\theta_i,\boldsymbol\psi,\boldsymbol{y})}}\\
& \propto \frac{p\left(\boldsymbol\theta, \boldsymbol\psi\mid\boldsymbol{y}\right)}{p(\boldsymbol\theta_{-i}\mid\theta_i,\boldsymbol\psi,\boldsymbol{y})} \propto \frac{p(\boldsymbol\psi)p(\boldsymbol\theta\mid\boldsymbol\psi)p(\boldsymbol{y}\mid\boldsymbol\theta,\boldsymbol \psi)}{p(\boldsymbol\theta_{-i}\mid\theta_i,\boldsymbol\psi,\boldsymbol{y})}\\
&\approx \left. \frac{p(\boldsymbol\psi)p(\boldsymbol\theta\mid\boldsymbol\psi)p(\boldsymbol{y}\mid\boldsymbol\theta,\boldsymbol \psi)}{\tilde{p}(\boldsymbol\theta_{-i}\mid\theta_i,\boldsymbol\psi,\boldsymbol{y})}\right|_{\boldsymbol\theta_{-i}={\boldsymbol\theta}^*_{-i}(\theta_i,\boldsymbol\psi)} =: \class{orange}{\tilde{p}(\theta_i\mid\boldsymbol\psi,\boldsymbol{y})}
\end{align*}

-->


# INLA &ndash; in a nutshell...

.pull-left[
.medium[
<ol style="counter-reset: my-counter 0;">
<li>Select a grid of \(H\) points \(\{\bm\psi_h^*\}\) and area weights \(\{\Delta_h\}\); interpolate the density to approximate to the posterior</li>
</ol>
]

`r vspace("18px")`
`r include_fig("inla1-1.png",width="80%",title="")`
]

--

.pull-right[
.medium[
<ol style="counter-reset: my-counter 1;">
<li> Approximates the conditional posterior of each \(\theta_j\), given \(\bm\psi, \bm{y}\) on the \(H−\)dimensional grid</li>
</ol>
]

`r vspace("50px")`
`r include_fig("inla2-1.png",width="85%",title="")`
]

---

count: false
# INLA &ndash; in a nutshell...

.pull-left[
.medium[
<ol style="counter-reset: my-counter 2;">
<li> Weight the conditional marginal posteriors by the density associated with each \(\psi_h^*\)
`r vspace("20px")`
</li>
</ol>
]

`r vspace("10px")`

`r include_fig("inla3-1.png",width="85%",title="")`
]

--

.pull-right[
.medium[
<ol style="counter-reset: my-counter 3;">
<li> (Numerically) sum over all the conditional densities to obtain the marginal posterior for \(\theta_j\)
`r vspace("50px")`
</li>
</ol>
]

`r vspace("-10px")`
`r include_fig("inla4-1.png",width="85%",title="")`
]

---

# Integrated Nested Laplace Approximation (INLA)

So, it's all in the name... 

`r vspace("20px")`

.large[Integrated Nested .myblue[**Laplace Approximation**]]

`r vspace("20px")`

- .myblue[Because Laplace approximation is the basis to estimate the unknown distributions]

---

count:false

# Integrated Nested Laplace Approximation (INLA)

So, it's all in the name... 

`r vspace("20px")`

.large[Integrated .myblue[**Nested**] Laplace Approximation]

`r vspace("20px")`

- Because Laplace approximation is the basis to estimate the unknown distributions

`r vspace("20px")`

- .myblue[Because the Laplace approximations are nested within one another

  - Since (1.) is needed to estimate (2.)
  
  - NB: Consequently the estimation of (1.) might not be good enough, but it can be refined (eg using a finer grid)
]

---

count:false

# Integrated Nested Laplace Approximation (INLA)

So, it's all in the name... 

`r vspace("20px")`

.large[.myblue[**Integrated**] Nested Laplace Approximation]

`r vspace("20px")`

- Because Laplace approximation is the basis to estimate the unknown distributions

`r vspace("20px")`

- Because the Laplace approximations are nested within one another

  - Since (1.) is needed to estimate (2.)
  
  - NB: Consequently the estimation of (1.) might not be good enough, but it can be refined (eg using a finer grid)

`r vspace("20px")`

- .myblue[Because the required marginal posterior distributions are obtained by (numerical) integration.]

---

name: Package

`r vspace("250px")`

.myblue[.center[.huge[
**The `R-INLA` package**]]]

---

# The `INLA` package for `R`

- Good news is that all the procedures needed to perform INLA are implemented in a `R` package. This is effectively made by two components

`r vspace("20px")`

--

1\. The .red[`GMRFLib`] library

  - A `C` library for fast and exact simulation of GMRFs

--

2\. The .blue[`inla`] program
  
  - A standalone `C` program build upon the `GMRFLib` library (it performs the relevant computation and returns the results in a standardised way)

`r vspace("20px")`

--

**NB**: Because the package `R-INLA` relies on a standalone `C` program (and other reasons...), it is not available directly from `CRAN`.

`r vspace("20px")`


`R-INLA` runs natively under Linux, Windows and Mac and it is possible to do multi-threading using `OpenMP`

---

# The `INLA` package `R` - Installation

- From `R`, installation of the stable version is performed typing 

`install.packages("INLA",repos=c(getOption("repos"),INLA="https://inla.r-inla-download.org/R/stable"), dep=TRUE)`

`r vspace("20px")`

- Later, you can upgrade the package by typing 

`library(INLA)`

`inla.upgrade()`

`r vspace("20px")`

- A test-version (which may contain unstable updates/new functions) can be obtained by typing 

`inla.upgrade(testing=TRUE)`

`r vspace("20px")`

- Type `inla.version()` to find out the installed version

---

# Step by step guide to using `R-INLA`

1\. The first thing to do is to .red[**specify the model**]

  - For example, assume we have a generic model
.myblue[
\begin{align*}
y_i & \stackrel{\small{iid}}{\sim} p(y_i \mid \theta_i) \\
\eta_i & =   \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + f(z_i)
\end{align*}
]

where

- $\color{olive}{\bm x = (\bm x_1,\bm x_2)}$ are observed covariates 

- $\color{olive}{\bm \beta = (\beta_0,\beta_1,\beta_2)\sim \mbox{Normal}(0,\tau_1^{-1})}$ are unstructured (*fixed*) effects 

- $\color{olive}{\bm z}$ is an **index**. This can be used to include structured (*random*), spatial, spatio-temporal effect, etc.

- $\color{olive}{f \sim \mbox{Normal}(0,\bm{Q}^{-1}_f(\tau_2))}$ is a suitable function used to model the structured effects

--

`r vspace("20px")`

- As mentioned earlier, this formulation can actually be used to represent quite a wide class of models!


---

# Step by step guide to using `R-INLA`

- The model is translated in `R` code using a .red[`formula`]

- This is sort of standard in `R` (you would do pretty much the same for calls to functions such as `lm` or `glm`)

  .olive[`formula = y `] $\color{olive}{\sim}$ .olive[`1+  x1 + x2 + f(name=z, model="...",hyper=...)`]

--

`r vspace("-10px")`

- The .olive[`f()`] function can account for several structured nonlinear effects. We have for example:

  - .olive[`iid`] specify independent random effects
  
  - .olive[`rw1`], .olive[`rw2`], .olive[`ar1`] are smooth effect of covariates or time effects
  
  - .olive[`besag`] models spatially structured effects (CAR)
    
  - .olive[`generic`] is a user-defined precision matrix
    
- Type .olive[`inla.list.models("latent")`] for the complete list and find descriptions at .olive[[https://www.r-inla.org/documentation](https://www.r-inla.org/documentation)]

- Some options can be specified for the .olive[`f()`] term: for example .olive[`hyper`] is used to specify the prior on the hyperparameters (more later).

--

`r vspace("-10px")`

- It is possible to include in the `formula` several .olive[`f()`]  terms specifying them separately, e.g. 

.olive[`formula <- y`] $\color{olive}{\sim}$ .olive[`x1 + x2 + f(z1,...) + f(z2,...) + f(z3,...)`]


---

# Step by step guide to using `R-INLA`

2\. Call the function .olive[`inla`] to fit the model, specifying the data and options (more on this later),e.g.

.olive[`m = inla(formula, family="...", data=data.frame(y,x1,x2,z))`]

`r vspace("20px")`
--

-- 

- The data need to be included in a suitable .olive[`data.frame`]

`r vspace("20px")`
--

- The distribution of the data (i.e. the likelihood) is specified with the .olive[`family`] option.

  - Type .olive[``inla.list.models("likelihood")`] for the complete list of likelihood function (we have .olive[`poisson, binomial, gamma, beta, gaussian`]  and many others).
  
`r vspace("20px")`
--

  -  Visit .olive[[https://www.r-inla.org/documentation](https://www.r-inla.org/documentation)] for the complete description

---

# Step by step guide to using `R-INLA`

The .olive[`control.xxx=list(...)`] statements  in the .olive[`inla`] function control various part of the INLA program:

- .olive[`control.compute`]: for computing measures of fit (eg DIC)

`r vspace("20px")`

- .olive[`control.predictor`]: for specifying the *Observation matrix* $\bm A$ which links the latent field to the data 

`r vspace("20px")`

- .olive[`control.family`]: for changing the prior distribution of the likelihood hyperparameters

`r vspace("20px")`

- .olive[`control.fixed`]: for changing the prior distribution of the fixed effects

`r vspace("20px")`

- .olive[`control.inla`]: for changing the strategy to use for the approximations ('gaussian', 'simplified.laplace' (default) or 'laplace') or the grid exploration strategy

`r vspace("20px")`

- and many others for expert use.

---

# Step by step guide to using `R-INLA`

`R` returns an object .olive[`m`] in the class `inla`, which has some methods available as for example .olive[`summary()`]  and  .olive[`plot()`].

`r vspace("20px")`

Some of the elements in an `inla` object returned by a call to `inla()` are:

`r include_fig("Table_out_inla.png", width="60%",title="")`

.center[Source: `r Citet(bibfile,"gomez2020bayesian")`, Section 2.3.1]

---

#Toy example

We consider the .olive[`iris`] dataset included in the .olive[`R`] datasets (see .olive[`?iris`]), regarding the measurements in centimeters of the variables *sepal length* and *width* and *petal length* and *width*, respectively, for 50 flowers from each of 3 species of iris. The species are Iris setosa, versicolor, and virginica. See Section 2.6 of the INLA book.

```{r iris_summary, echo=TRUE, eval=TRUE}
summary(iris)
```

We specify a simple regression model with `Petal.length` and `Petal.width` as dependent and independent variables, respectively:

\begin{align*}
\text{ Petal.length}_i&\sim \text{Normal}(\eta_i,1/\sigma^2_e)\\
\eta_i &= \beta_0 + \beta_1\text{ Petal.width}_i
\end{align*}

---

# Toy example: run INLA + exploring the fixed effects output
`r vspace("-15px")`

```{r iris_inla, echo=TRUE, eval=TRUE}
library(INLA)
formula <- Petal.Length ~  1 + Petal.Width
output <- inla(formula, family="gaussian", data=iris)
```




```{r iris_fixed, echo=TRUE, eval=TRUE}
output$summary.fixed
```

- For each unstructured *fixed* effect, `R-INLA` reports a set of summary statistics from the posterior distribution.

--

- The value of the Kullback-Leibler divergence `kld` describes the difference between the Gaussian approximation and the Simplified Laplace Approximation (SLA) to the marginal posterior densities:

  - Small values indicate that the posterior distribution is well approximated by a Normal distribution  
  - If so, the more sophisticated SLA gives a *good* error rate and therefore there is no need to use the more computationally intensive *full* Laplace approximation.

---

# Exploring the output: hyperparatemers

```{r iris_hyper, echo=TRUE, eval=TRUE, tidy=TRUE}
output$summary.hyperpar
```

- For each hyperparameter the summary statistics are reported to describe the posterior distribution.

- **NB**: `INLA` reports results on the **precision** scale (more on this later).


---

# Manipulating the marginals from `R-INLA`: fixed effects

.panelset[
.panel[.panel-name[marginals1]
.pull-left[
```{r explore_fix, echo=TRUE, eval=TRUE, out.width="50%" }
names(output$marginals.fixed)
beta1_post <-output$marginals.fixed[[2]]
marg <- inla.smarginal(beta1_post)
q <-inla.qmarginal(0.05,beta1_post)
```

```{r explore_plot, echo=TRUE, eval=FALSE}

plot(marg,t="l",
     ylab="",xlab="", 
     main=expression(paste("p(",beta[1], "| y)")))
polygon(c(marg$x[marg$x <= q ], q),
        c(marg$y[marg$x <= q ], 0),
        col = "slateblue1", border = 1)
```
]

.pull-right[
```{r explore_plot_out, echo=FALSE, ref.label="explore_plot", fig.dim=c(4.5, 4.0), out.width="100%"}
plot(inla.smarginal(beta1_post),t="l", ylab="",xlab="")
polygon(c(marg$x[marg$x <= q ], q),
        c(marg$y[marg$x <= q ], 0),
        col = "slateblue1",
        border = 1)
```
]
]

.panel[.panel-name[marginals2]
.small[.pull-left[
```{r explore_fix1, echo=TRUE, eval=TRUE, out.width="50%" }
inla.pmarginal(q,beta1_post)
d <-inla.dmarginal(q,beta1_post)
d
inla.rmarginal(4, beta1_post)
```
```{r explore_plot1, echo=FALSE, eval=FALSE}
plot(inla.smarginal(beta1_post),t="l",
     ylab="",xlab="", main=expression(paste("p(",beta[1], "| y)")))
abline(v=q, lty=2)
abline(h=d, lty=2)
```
]
]

.pull-right[
```{r explore_plot1_out, echo=FALSE, ref.label="explore_plot1", fig.dim=c(4.5, 4), out.width="100%"}
plot(inla.smarginal(beta1_post),t="l",
     ylab="",xlab="")
abline(v=q)
abline(h=d)
```
]
]
]

---

# Manipulating the marginals from `R-INLA`: hyperparameters

`INLA` works with the precision by default

.panelset[
.panel[.panel-name[Precision]
.pull-left[
```{r explore_hyp, echo=TRUE, eval=TRUE, out.width="50%", tidy=TRUE}
names(output$marginals.hyperpar)
prec_post <-output$marginals.hyperpar[[1]]
```

```{r explore_plot_hyp, echo=TRUE, eval=FALSE}

plot(inla.smarginal(prec_post),t="l",
     ylab="",xlab="", 
     main=expression(paste("p(",1/sigma[e]^2, "| y)")))
```
]

.pull-right[
```{r explore_plot_hyp_out, echo=FALSE, ref.label="explore_plot_hyp", fig.dim=c(4.5, 4.0), out.width="100%"}
plot(inla.smarginal(prec_post),t="l", ylab="",xlab="")
```
]
]

.panel[.panel-name[Variance]
.pull-left[
```{r explore_hyp1, echo=TRUE, eval=TRUE, out.width="50%" }
var_post = inla.tmarginal(fun=function(x) 
            1/x, mar=prec_post)
inla.emarginal(fun=function(x) 1/x, 
marg=prec_post)
```

```{r explore_plot_hyp1, echo=TRUE, eval=FALSE}
plot(inla.smarginal(var_post),t="l",
     ylab="",xlab="", 
     main=expression(paste("p(",sigma[e]^2, "| y)")))
```
]

.pull-right[
```{r explore_plot_hyp1_out, echo=FALSE, ref.label="explore_plot_hyp1", fig.dim=c(4.5, 4), out.width="100%"}
plot(inla.smarginal(var_post),t="l",
     ylab="",xlab="")
```
]
]
]


---

#Summary

The INLA approach is not a rival/competitor/replacement to/of MCMC, just a better option for the class of LGMs.

`r vspace("20px")`

- The basic idea behind the INLA procedure is simple

  - Repeatedly use Laplace approximation and take advantage of computational simplifications due to the structure of the model

  - Use numerical integration to compute the required posterior marginal distributions

`r vspace("20px")`

- Complications are mostly computational and occur when 

  - Extending to a large number of hyperparameters

  - Markedly non-Gaussian observations

---

# References

```{r refs, echo=FALSE, results="asis"}
PrintBibliography(bibfile,.opts=list(max.names=3))
```
