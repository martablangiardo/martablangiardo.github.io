---
title: "Session 2.2: Posterior Predictive Distribution and Monte Carlo computation"
params: 
   conference: "Bayesian modelling for Spatial and Spatio-temporal data"
   location: "Imperial College"
   date: January-March 2023
   short_title: "MSc in Epidemiology"
output:
   xaringan::moon_reader: 
    includes: 
       # This line adds a logo based on the format selected in the file 'assets/include_logo.html'
       in_header: "assets/latex_macros.html" 
       # NB: the actual options (eg placement of the logo and actual logo file) can be changed there
     #  after_body: "assets/insert-logo.html" # Monica commented this
    seal: false
    yolo: no
    lib_dir: libs
    nature:
      beforeInit: ["assets/remark-zoom.js","https://platform.twitter.com/widgets.js"]
      navigation:
        scroll: false # disable slide transitions by scrolling
      highlightStyle: github
      highlightLines: yes
      countIncrementalSlides: no
      ratio: '16:9'
      titleSlideClass:
      - center
      - middle
    self_contained: false 
    css:
    - "assets/beamer.css"
editor_options: 
  chunk_output_type: console
---

```{r global_options, echo = FALSE, include = FALSE}
options(width = 999)
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE,
                      cache = FALSE, tidy = FALSE, size = "small")
#options(htmltools.preserve.raw = FALSE)
# https://stackoverflow.com/questions/65766516/xaringan-presentation-not-displaying-html-widgets-even-when-knitting-provided-t
```

```{r echo=F, message=FALSE, warning=FALSE, comment=NA}
source("assets/setup.R")
library(INLA)
xaringanExtra::use_panelset()
bibfile=RefManageR::ReadBib("~/Dropbox/TEACHING/YEAR_2023/Bayes_Spatial_2023/Material/Biblio_new.bib",check = FALSE)
```

class: title-slide

# `r rmarkdown::metadata$title``r vspace("10px")` `r rmarkdown::metadata$subtitle`

## `r rmarkdown::metadata$author`

### `r rmarkdown::metadata$institute`    

### `r rmarkdown::metadata$params$conference`, `r rmarkdown::metadata$params$location` 

<!-- Can also separate the various components of the extra argument 'params', eg as in 
### `r paste(rmarkdown::metadata$params, collapse=", ")`
-->

`r ifelse(is.null(rmarkdown::metadata$params$date),format(Sys.Date(),"%e %B %Y"),rmarkdown::metadata$params$date)`

---

layout: true  

.my-footer[ 
.alignleft[ 
&nbsp; &copy; Marta Blangiardo | Monica Pirani
]
.aligncenter[
`r rmarkdown::metadata$params$short_title` 
]
.alignright[
`r rmarkdown::metadata$params$conference`, `r short_date` 
]
] 

```{css,echo=FALSE, eval=FALSE}
.red {
  color: red;
}
.blue {
  color: 0.14 0.34 0.55;
}

.content-box-blue { background-color: #F0F8FF; }

}

.scrollable {
  height: 80%;
  overflow-y: auto;
}

```
<style>
pre {
  overflow-x: auto;
}
pre code {
  word-wrap: normal;
  white-space: pre;
}
</style>


---

# Learning objectives

After this lecture you should be able to 
`r vspace("40px")`
- Explain what is the posterior predictive distribution   
`r vspace("40px")`
- Explain how it is computable
`r vspace("40px")`
- Describe Monte Carlo computation and why it is useful
`r vspace("40px")`
ADD MC objectives


---

# Outline 

`r vspace("30px")`

1\. [Bayesian prediction](#Bayesian prediction)

`r vspace("30px")`

2\. [Computation of PPD](#Computation of PPD)

`r vspace("30px")`

3\. [Example](#Example)

`r vspace("30px")`

a\. [Monte Carlo simulations](#Monte Carlo simulations)


---

name: Bayesian prediction
  
`r vspace("250px")`

.myblue[.center[.huge[
**Posterior Predictive Distribution**]]]

---

# Bayesian prediction


- Often the objective of our analysis is to predict a future event

`r vspace("20px")`
--

- Consider this example: 

`r vspace("20px")`

We estimate the prevalence of a disease in a UK hospital using a sample of n = 58 individuals. 
`r vspace("10px")`
We find that $y = 10$ individuals have the disease. 
`r vspace("10px")`
.red[What is the probability that, if we additionally sample $k = 30$ individuals this year, at least 5 will have the disease?]

--

`r vspace("20px")`

- As usually we start specifying the data distribution:

$$y \sim \mathrm{Binomial}(\theta,n=58)$$


- Let's  $\theta$ be the true disease prevalence and $y^{*}$ be the .red[predicted value] 

--

- If $\theta$ were known, then we would predict $$y^*|\theta\sim \mbox{Binomial}(30,\theta)$$
	 thus $\mbox{P}(y\geq{5}) = 1-(\sum_{j=0}^4 \theta^{j}(1-\theta)^{30-j})$
	 
.center[.content-box-blue[BUT ... $\theta$ is unknown]]

---


# Source of variation in prediction:

- We don't know the true value of the parameters and we specify a prior on it:

$$\theta \sim \mathrm{Beta}(a,b)$$

- There is sampling variability ( $\rightarrow$ choice of the data distribution)


--


To account for the sources of variation we iterate the following steps:


1. Sample from the posterior distribution $\theta \sim p(\theta\mid y)$

2. Sample new values $y^*\sim p(y\mid \theta)$

- By repeating these steps a large number of times, we eventually obtain a reasonable approximation to the .red[posterior predictive distribution].

---

# Posterior Predictive Distribution (PPD)

- The .red[PPD] represents our uncertainty over the outcome of a future data collection, accounting for the observed data and model choice


- For the sake of prediction, the parameters are not of interest. They are vehicles by which the data inform about the predictive model


- The .red[PPD] averages over their posterior uncertainty

	$$p(y^*|y) = \int p(y^*|\theta)p(\theta|y)d\theta$$
- This properly accounts for parametric uncertainty


- The input is data, the output is a prediction distribution


---

name: Computation
  
`r vspace("250px")`

.myblue[.center[.huge[
**Computation**]]]


---
# Computing the PPD


- Say $\theta^{(1)},...,\theta^{(M)}$ are samples from the posterior


- If we make a sample for $y^*$ for each $\theta^{(m)}$, $$y^{*(m)}\sim p(y|\theta^{(m)})$$ then the $y^{*(m)}$ are samples from the PPD


- The posterior predictive mean is approximated by the sample mean of the $y^{*(m)}$


- The probability that $y^{*}\geq 5$ is approximated by the sample proportion of the $y^{*(m)}$ that are equal or above 5


---

name: Example
  
`r vspace("250px")`

.myblue[.center[.huge[
**Example**]]]

---

# Example

- We estimate the prevalence of a disease in the UK population using a sample of $n=58$ individuals.

- We find that $y=10$ individuals have the diseases. 

- .red[What is the probability that, if we additionally sample $k=30$ individuals this year, at least 5 will have the disease?]
.pull-left[
1. .blue[Likelihood]: $y \sim \mbox{Binomial}(\theta,58)$
2. .blue[Prior]: $\theta \sim \mbox{Beta}(1,1)$ 
3. .blue[Posterior]: $\theta \mid y \sim \mbox{Beta}(10+1,58-10+1)$
4. .blue[PPD]: $y^* \sim \mbox{Binomial}(\theta \mid y,30)$
5. $P(y\geq{5}) = \sum_{j=5}^{30} P(y^*=j)$
]

.pull-right[
`r include_fig("PPD_example.png",width="60%",title="")`
]

`r vspace("-100px")`
```{r eval=TRUE, echo=FALSE,  fig.height = 3.8, fig.width = 4, fig.show='hold',opts=list(width="40%")}
set.seed(1234)
theta = rbeta(1000,11,49)
y = c()
for(i in 1:1000){
  y[i] = rbinom(1,30,theta[i])
}
p = sum(y>=5)

hist(y, col="yellow", main="")
abline(v=5, lty=2,lwd=1)
```


---

name: Monte Carlo simulations
  
`r vspace("250px")`

.myblue[.center[.huge[
**Monte Carlo simulations**]]]

---
# Monte Carlo simulations

- Monte Carlo (MC) sampling is based on the idea that if you have a large random sample from a certain distribution, the statistics that you can calculate in this sample (mean, standard deviation, percentiles...) will be very similar to the corresponding theoretical values in the distribution.


- If you have a complicated mathematical expression for a distribution and you cannot calculate algebraically important parameters, you could get the computer to generate a large random sample from such a distribution.


- By calculating the mean of that parameter in the sample you could estimate the mean in the original distribution with great precision.

---

# Example: a Monte Carlo approach to estimating tail-areas of distributions

- Suppose we want to know the probability of getting 8 or more heads when we toss a fair coin 10 times.


- An algebraic approach would be:

  - $y$ number of heads
  
  - $y \sim Binomial(\theta, n)$
  
  - $P(y>=8)$
  
---
# Example in R

- Let's assume that $y=7$ and $n=10$. In R, we first we estimate the parameters of the posterior $P(\theta \mid y)$

```{r, eval=TRUE, echo=TRUE, results='hide'}
# Prior parameters
y = 7
n = 10
a = 1
b = 1

# Posterior parameters
a1 = a + y
b1 = b + n - y
```

- With this setting the exact posterior distribution of $\theta$ is
$Beta(a_1=1+7,b_1=1+10-7)$


- To approximate the log-odds, we simulate $M=50000$ values from this Beta posterior distribution using the `rbeta` function.

```{r, eval=TRUE, echo=TRUE, results='hide'}
M = 50000
sim = rbeta(M, shape1=a1, shape2=b1)
logodds = log(sim/(1-sim))
```


---
# Results and comparison with the theoretical distribution

The empirical distribution of the Monte Carlo sample is plotted below together with the exact posterior distribution of $\theta$

`r include_fig("MC_r.jpg",width="70%",title="")`

